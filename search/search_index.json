{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GOVSocial!","text":"<p>GovSocial's mission is to provide instances of Fediverse social media platforms \"safe\" for use by public service employees in Federal, State, tribal, and local government, public education, and other publicly-funded organizations.</p> <p>As public servants ourselves, we believe in the US Digital Service Playbook principle of \"Default to Open\", so this site provides links to how we build and run our instances, and transparent reporting on the operating performance and costs of our instances.</p>"},{"location":"#what-to-expect-when-you-join","title":"What to expect when you join","text":"<p>Our instance federation and moderation polices are stricter than many general use instances.</p> <p>It is a requirement that you sign up with your work email address to verify your identity as a public service employee. .gov email addresses are automatically approved, while others will be manually approved, based on cross-checking the domain part of the email address with that of the organization's public website, and confirming that the organization is a public service organization.</p> <p>You must also use your real name, and the use of official headshots or logos for account avatars is strongly encouraged. You are also encouraged to use your profile to provide links (verified, if possible) back to the appropriate pages on your organization's website, your LinkedIn profile, and any others that provide confidence to other users that you are who you say you are.</p> <p>All content must be appropriate for your work environment, and in keeping with your presence on our instances as a verified public service employee. You may also want to check with your organization's social media policy before registering.</p> <p>Organizational accounts and bots are encouraged, provided they meet the same requirements as individual accounts, and follow the instance rules.</p> <p>We hope you enjoy your time here. Please reach out to the instance team if you have any questions or concerns. Have fun!</p>"},{"location":"building/","title":"Building GOVSocial","text":"<p>When we first began thinking of hosting Fediverse platforms for public service use, one of our first considerations was whether we would self-host or host in the cloud, and whether we wanted to deploy on servers or in a containerized environment.</p> <p>We wanted to start small, and scale quickly as needed without service interruption. We started the project in November 2022, just as the first wave of refugees from commercial platforms started, and were keenly aware of the struggles of existing instances to rapidly scale to meet demand and to control the escalating hosting costs of a large influx of users.</p> <p>This need for scalable reliability and control over our hosting costs led us to pick a Kubernetes deployment hosted in a major cloud service provider. This met the US Digital Service Playbook principles of \"Choose a modern technology stack\" and \"Deploy in a flexible hosting environment\".</p> <p>We picked Google primarily because they are committed to open standards and have a proven track record of support for the public sector.</p> <p>We set out to prove that we could implement our entire platform using GCP services. As you will see, we managed that with the sole exception of transactional email, as Google does not currently provide such a service, at least at any sort of scale.</p> <p>The project is indebted to two excellent guides on hosting our first instance, Mastodon, on Kubernetes and on Google Cloud Platform (GCP):</p> <ul> <li>The Funky Penguin's Geek Cookbook</li> <li>Justin Ribeiro's guide to installing Mastodon on GCP</li> </ul> <p>We made quite a few necessary modifications to both of these, and learned a ton along the way. We have documented our journey here to assist others interested in a similar implementation.</p>"},{"location":"building/cluster/","title":"Creating the cluster","text":"<p>None of the How-Tos we found covered the creation of the initial Kubernetes cluster, because this is really down to the choice of hosting environment, and appetite for cost. We created a standard cluster in GKE, with the following cost-saving caveats:</p> <ul> <li>As tempting as it sounds from an ease of management perspective, do NOT create an AutoPilot cluster. The minimum vCPU and Memory requests for an AutoPilot cluster will bankrupt you before you start (roughly $200/month!).</li> <li>Use spot VMs for the nodes on which your cluster will be deployed. They offer significant cost savings over standard VMs and allow you to use decent-sized machines for your nodes at a fraction of the cost. We chose the e2-medium shared core machines, and set the cluster to auto-scale from 0 to 3 nodes. You will want to make sure to set a Pod Disruption Budget (PDB) to make sure that auto-updating and pre-emption of the Spot VM nodes doesn't disrupt your instance services. We took the advice given here to set <code>maxSurge=1 maxUnavailable=0</code>.</li> <li>Start with the smallest cluster available. You can always add more and beefier nodes to it later.</li> </ul> <p>IMPORTANT: You will need to make sure you create your cluster with Workload Identity enabled. Authenticated access from the cluster service account (which is NOT an IAM service account) to other GCP services such as Cloud Storage depends on it.</p>"},{"location":"building/domains/","title":"DNS Domains","text":"<p>You will need a public DNS domain for your Fediverse instances. Because we wanted to host more than one Fediverse platform under the GOVSocial umbrella, we choose <code>govsocial.org</code> as our root domain, with <code>{instance}.govsocial.org</code> as the instance-specific hostname.</p> <p>For Mastodon, there are a couple of things to be aware of when choosing a domain in general, and this configuration in particular:</p> <ul> <li>Be aware of recent (December 21, 2022) changes to the Mastodon Trademark Policy. You will need written permission from Mastodon gGmbH to use 'Mastodon' or any derivatives (e.g. \"mastodoon\", \"mast0don\", \"mstdn\") in your domain or hostname1.</li> <li>We wanted to have our user accounts use <code>@govsocial.org</code> for all our instances instead of the actual instance hostname. This has implications for how both Mastodon and our load balancer are configured.</li> </ul> <p>Register your domain (we used Google Domains), set Google Cloud DNS as the nameserver, and enable DNSSEC. The console will guide you through the steps outlined here.</p> <p>If you are fine with manually creating DNS records for your platforms (which is what we do), you're done at this point.</p>"},{"location":"building/domains/#externaldns","title":"ExternalDNS","text":"<p>The The Funky Penguin's Geek Cookbook implements ExternalDNS to manage the DNS entries required for the Fediverse instances. We really tried to get this to work (and pretty much did), but in the end, it was a lot of trouble-shooting to get it working on GKE with Cloud DNS, we were unable to get it to manage everything we needed without resorting to manual entry anyway, and it's not as if your DNS entries should change often enough to make it either necessary or worthwhile.</p> <p>However, if you insist, here are some tips we learned during our efforts to get it working.</p>"},{"location":"building/domains/#service-account","title":"Service Account","text":"<p>One of the challenges about deploying in different components of a cloud service provider is granting authenticated access between these components, and particularly between services running in pods on your cluster, and other services such as Cloud DNS.</p> <p>The service accounts inside a GKE cluster are not part of the GCP IAM system that controls access to these other services. To grant services running GKE pods access to those services, the GKE service account in a pod needs to be mapped to an IAM account with the correct roles.</p> <p>The first thing to do is install the <code>IAM Service Account Credentials API</code> into your GCP project. You can do this by selecting the <code>IAM &amp; Admin</code> item on the GCP Console menu. If the API is not already installed, you will be prompted to add it. Once it is installed, navigate to <code>Service Accounts</code>, and create a new service account principal. You will need to assign this account the <code>DNS Administrator</code> role.</p> <p>Next, you will need to map the GKE service account in the ExternalDNS pod to this IAM service account. The instructions for doing that are here. The GKE service account for ExternalDNS will be <code>external-dns</code>, if you are following The Funky Penguin's Geek Cookbook.</p>"},{"location":"building/domains/#custom-resource-definitions-crds","title":"Custom Resource Definitions (CRDs)","text":"<p>If you are using CRDs to manage your DNS records (again, from the Cookbook), you will likely encounter this error:</p> <pre><code>error listing resources in GroupVersion \\\"externaldns.k8s.io/v1alpha1\\\": the server could not find the requested resource\n</code></pre> <p>To fix this, run this from your CLI machine:</p> <pre><code>~ \u276f kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/external-dns/master/docs/contributing/crd-source/crd-manifest.yaml\n</code></pre> <p>You will also likely encounter this error:</p> <pre><code>DNSEndpoint (\"dnsendpoints.externaldns.k8s.io is forbidden: User \\\"system:serviceaccount:external-dns:external-dns\\\" cannot list resource \\\"dnsendpoints\\\" in API group \\\"externaldns.k8s.io\\\" at the cluster scope\")\n</code></pre> <p>To this this one, run this from your CLI machine:</p> <pre><code>~ \u276f kubctl get clusterrole external-dns-external-dns -o yaml &gt; {file}\n</code></pre> <p>Edit the file (with <code>vi</code> or your editor of choice - not getting into that debate here!) to add the following lines at the end:</p> <pre><code>- apiGroups: [\"externaldns.k8s.io\"]\nresources: [\"dnsendpoints\"]\nverbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"externaldns.k8s.io\"]\nresources: [\"dnsendpoints/status\"]\nverbs: [\"*\"]\n</code></pre> <p>Then apply the new configuration with:</p> <pre><code>~ \u276f kubectl apply -f {file}\n</code></pre> <p>Are we having fun yet?</p> <p>Good, because we're not done.</p>"},{"location":"building/domains/#record-ownership-tracking","title":"Record Ownership Tracking","text":"<p>You will experience conflicts between the default <code>TXT</code> DNS records that ExternalDNS uses to track the DNS records it manages, and any actual <code>TXT</code> records that you need (for DMARC, for example). You will need to set the <code>txtSuffix</code> value to <code>\"txt\"</code> in the external-dns overrides file to avoid this issue. <code>txtPrefix</code> doesn't seem to work.</p> <p>Even after all this, we were unable to get two things to work at all:</p> <ul> <li>An <code>A</code> record for our \"naked domain\"</li> <li>The <code>MX</code> and <code>TXT</code> records necessary for our custom domain to work with AWS SES.</li> </ul> <p>So we gave up, and reverted to manual DNS record entry. YMMV.</p> <ol> <li> <p>GOVSocial obtained this permission for our use of <code>mastodon.govsocial.org</code> on January 10, 2023\u00a0\u21a9</p> </li> </ol>"},{"location":"building/fluxhelm/","title":"Flux and Helm","text":"<p>One of the best decisions we made as we started our first platform install (Mastodon), was to follow the advice in The Funky Penguin's Geek Cookbook and use Flux to bootstrap our cluster and deploy Helm charts and Flux kustomizations from a private GitHub repo. Doing this made it relatively easy to install what we needed from existing Helm charts. It also allowed us to quickly rebuild our cluster (something we did at least twice!) as we made poor configuration choices and mistakes while we figured things out.</p>"},{"location":"building/fluxhelm/#install-flux-cli","title":"Install Flux-CLI","text":"<p>To start, you will need to install the Flux CLI. Rather than install locally, we spun up an <code>e2-micro</code> VM in Google Compute Engine to host this, <code>kubectl</code>, and the other tools we would need for our build. Rather than messing with the Docker image, we installed the binaries with:</p> <pre><code>~ \u276f curl -s https://fluxcd.io/install.sh | sudo bash\n</code></pre> <p>NOTE: To run <code>flux-cli</code> from a Compute Engine VM, your <code>Compute Engine default service account</code> needs the <code>Kubernetes Engine Admin</code> role. You can edit the roles assigned to service account principals in IAM. You should also get used to running this each time you connect to your VM:</p> <pre><code>~ \u276f gcloud container clusters get-credentials {cluster_name} --region {region}\n</code></pre> <p>This will authenticate you to your cluster.</p> <p>You should also install the <code>kubectl</code> and <code>google-cloud-sdk-gke-gcloud-auth-plugin</code> packages at this point, from your machine's package manager. Don't forget to periodically run <code>sudo apt-get update &amp;&amp; sudo apt-get upgrade</code> on your machine to keep your packages up to date - the Google Cloud packages update quite often.</p>"},{"location":"building/fluxhelm/#bootstrap-flux-on-the-cluster","title":"Bootstrap Flux on the Cluster","text":"<p>These instructions assume you are using a GitHub personal or organizational repo. Additional instructions for other git repositories can be found here.</p>"},{"location":"building/fluxhelm/#github-setup","title":"GitHub Setup","text":"<p>Before we bootstrap Flux on the cluster, a word about your GitHub repo setup. You will need a private GitHub personal or organizational repo. Make sure the repo is private, as your instance configurations, including secrets, will be stored there. When bootstrapped as detailed below, your repo will be used for all your Flux deployments, so avoid naming it for a particular cluster or platform.</p>"},{"location":"building/fluxhelm/#personal-access-token-pat","title":"Personal Access Token (PAT)","text":"<p>Flux uses a <code>classic</code> Personal Access Token (PAT) created for the personal GitHub account you use to access this repo. PATs are created and maintained in <code>&lt;&gt; Developer Settings</code> at the very bottom of the Settings menu for your account (accessed by clicking on your avatar at the top right of your GitHub window and choosing <code>Settings</code>). The PAT will need the full <code>repo</code> role. Make a note of the token (it will start with <code>gpg_</code>) - you will only be shown it once.</p> <p>By default, the PAT will expire every 30 days. You will get an email from GitHub 7 days before it expires. To rotate it:</p> <ul> <li>Delete the auth secret from the cluster with: <pre><code>~ \u276f kubectl -n flux-system delete secret flux-system\n</code></pre></li> <li>Rerun flux bootstrap with the same args as you use to set it up</li> <li>flux will generate a new secret and will update the deploy key if you\u2019re using SSH deploy keys (you will be). You should get another email from GitHub telling you that a new SSH key has been created, and you're all set.</li> </ul>"},{"location":"building/fluxhelm/#bootstrap-flux","title":"Bootstrap Flux","text":"<p>Bootstrap Flux on your cluster by running the following:</p> <p><pre><code>~ \u276f flux bootstrap github \\\n--owner={my-github-username} \\\n--repository={my-repository} \\\n--path=clusters/{my-cluster} \\\n--personal\n</code></pre> NOTE: If you are using an organizational repo, omit <code>--personal</code> from the options.</p> <p>You will be prompted for your PAT, which you can copy from where you stored it and paste into the command prompt before hitting <code>[Enter]</code> (you won't see anything when you paste it in).</p> <p>If all goes well, you should see the following by typing <code>kubectl get pods -n flux-system</code> at the command prompt:</p> <pre><code>~ \u276f kubectl get pods -n flux-system\nhelm-controller-564bf65b8b-fswms           1/1     Running     0\nkustomize-controller-7cbf46474d-5qxnw      1/1     Running     0\nnotification-controller-7b665fb8bd-t9vmr   1/1     Running     0\nsource-controller-84db8b78b9-xjkfm         1/1     Running     0\n</code></pre>"},{"location":"building/fluxhelm/#flux-repository-structure","title":"Flux Repository Structure","text":"<p>Having successfully carried out the above steps, you will now have started a Flux repo with the following structure plan (this will get built as we journey through the deployment of a platform):</p> <p><pre><code>/clusters\n  |/{cluster1}\n|/flux-system # &lt;-- This controls the Flux installation on the cluster\n|/[git|helm]repositories # &lt;-- This is where the Helm chart sources for the platforms are defined\n|/namespaces # &lt;-- This is where the k8s namespaces for the platforms are defined\n|/kustomizations\n        |kustomization-{platform1}.yaml #)     We create these when we deploy platforms like Mastodon\n|kustomization-{platform3}.yaml #) &lt;-- They tell flux to look in the matching folder in the\n|kustomization-{platform4}.yaml #)     repo root for instructions\n|/{cluster2}\n|/flux-system # &lt;-- This controls the Flux installation on the cluster\n|/[git|helm]repositories # &lt;-- This is where the Helm chart sources for the platforms are defined\n|/namespaces # &lt;-- This is where the k8s namespaces for the platforms are defined\n|/kustomizations\n        |kustomization-{platform2}.yaml #)     We create these when we deploy platforms like Mastodon\n|kustomization-{platform5}.yaml #) &lt;-- They tell flux to look in the matching folder in the\n|kustomization-{platform6}.yaml #)     repo root for instructions\n/{platform1} #)\n/{platform2} #)\n/{platform3} #) &lt;--We create these to hold configuration details for each platform\n/{platform4} #)\n/{platform5} #)\n/{platform6} #)\n</code></pre> This is an extremely powerful and flexible plan that allows different platforms to be deployed on multiple clusters from the same Flux repo.</p> <p>With all that set up, we can have some fun!</p>"},{"location":"building/postgresql/","title":"PostgreSQL","text":"<p>As with S3-compatible storage, most Fediverse platforms require a PostgreSQL database instance where much of the instance data is stored. Despite the importance of the performance of the database to that of the overall instance, you can save a lot of up-front cost by starting small and cloning to a larger machine as you grow.</p> <p>We used a Google CloudSQL PostgreSQL instance, selecting the smallest possible machine (<code>db-f1-micro</code>) and set the storage to <code>auto-grow</code>.</p> <p>You will need to ensure that you enable the Private IP interface for the instance so the GKE cluster can reach it. We did not enable the CloudSQL Auth Proxy - we may eventually do that at some point.</p>"},{"location":"building/storage/","title":"S3 Storage","text":"<p>Most Fediverse platforms require some form of S3-compatible storage for images and other media files.</p> <p>We used Google Cloud Storage for this. Your bucket will need to be <code>public</code>, with <code>fine-grained</code> access control, and use the <code>standard</code> storage class.</p> <p>The service account principal doesn't need any roles - you'll use a HMAC key for access.</p>"}]}