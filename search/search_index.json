{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GOVSocial!","text":"<p>GovSocial's mission is to provide instances of Fediverse social media platforms \"safe\" for use by public service employees in Federal, State, tribal, and local government, public education, and other publicly-funded organizations.</p> <p>As public servants ourselves, we believe in the US Digital Service Playbook principle of \"Default to Open\", so this site provides links to how we build and run our instances, and transparent reporting on the operating performance and costs of our instances.</p>"},{"location":"#what-to-expect-when-you-join","title":"What to expect when you join","text":"<p>Our instance federation and moderation polices are stricter than many general use instances.</p> <p>It is a requirement that you sign up with your work email address to verify your identity as a public service employee. .gov email addresses are automatically approved, while others will be manually approved, based on cross-checking the domain part of the email address with that of the organization's public website, and confirming that the organization is a public service organization.</p> <p>You must also use your real name, and the use of official headshots or logos for account avatars is strongly encouraged. You are also encouraged to use your profile to provide links (verified, if possible) back to the appropriate pages on your organization's website, your LinkedIn profile, and any others that provide confidence to other users that you are who you say you are.</p> <p>All content must be appropriate for your work environment, and in keeping with your presence on our instances as a verified public service employee. You may also want to check with your organization's social media policy before registering.</p> <p>Organizational accounts and bots are encouraged, provided they meet the same requirements as individual accounts, and follow the instance rules.</p> <p>We hope you enjoy your time here. Please reach out to the instance team if you have any questions or concerns. Have fun!</p>"},{"location":"building/","title":"Building GOVSocial","text":"<p>When we first began thinking of hosting Fediverse platforms for public service use, one of our first considerations was whether we would self-host or host in the cloud, and whether we wanted to deploy on servers or in a containerized environment.</p> <p>We wanted to start small, and scale quickly as needed without service interruption. We started the project in November 2022, just as the first wave of refugees from commercial platforms started, and were keenly aware of the struggles of existing instances to rapidly scale to meet demand and to control the escalating hosting costs of a large influx of users.</p> <p>This need for scalable reliability and control over our hosting costs led us to pick a Kubernetes deployment hosted in a major cloud service provider. This met the US Digital Service Playbook principles of \"Choose a modern technology stack\" and \"Deploy in a flexible hosting environment\".</p> <p>We picked Google primarily because they are committed to open standards and have a proven track record of support for the public sector.</p> <p>We set out to prove that we could implement our entire platform using GCP services. As you will see, we managed that with the sole exception of transactional email, as Google does not currently provide such a service, at least at any sort of scale.</p> <p>The project is indebted to two excellent guides on hosting our first instance, Mastodon, on Kubernetes and on Google Cloud Platform (GCP):</p> <ul> <li>The Funky Penguin's Geek Cookbook</li> <li>Justin Ribeiro's guide to installing Mastodon on GCP</li> </ul> <p>We made quite a few necessary modifications to both of these, and learned a ton along the way. We have documented our journey here to assist others interested in a similar implementation.</p>"},{"location":"building/cluster/","title":"Creating the cluster","text":"<p>None of the how-tos we found covered the creation of the initial Kubernetes cluster, because this is really down to the choice of hosting environment and appetite for cost. We created a standard cluster in GKE with the following cost-saving caveats:</p> <ul> <li>As tempting as it sounds from an ease of management perspective, do NOT create an AutoPilot cluster. The minimum vCPU and Memory requests for an AutoPilot cluster will bankrupt you before you start (roughly $200/month!).</li> <li>Use spot VMs for the nodes on which your cluster will be deployed. They offer significant cost savings over standard VMs and allow you to use decent-sized machines for your nodes at a fraction of the cost. We chose the e2-medium shared core machines and set the cluster to auto-scale from 0 to 3 nodes. You will want to make sure to set a Pod Disruption Budget (PDB) to make sure that auto-updating and pre-emption of the Spot VM nodes doesn't disrupt your instance services. We took the advice given here to set <code>maxSurge=1 maxUnavailable=0</code>.</li> <li>Start with the smallest cluster available. You can always add more and beefier nodes to it later.</li> </ul> <p>IMPORTANT: You will need to make sure you create your cluster with Workload Identity enabled. Authenticated access from the cluster service account (which is NOT an IAM service account) to other GCP services such as Cloud Storage depends on it.</p>"},{"location":"building/domains/","title":"DNS Domains","text":"<p>You will need a public DNS domain for your Fediverse instances if you want them to be accessible from the Internet. Because we wanted to host more than one Fediverse platform under the GOVSocial umbrella, we choose <code>govsocial.org</code> as our root domain, with <code>{instance}.govsocial.org</code> as the instance-specific hostname.</p> <p>For Mastodon, there are a couple of things to be aware of when choosing a domain in general, and this configuration in particular:</p> <ul> <li>Be aware of recent (December 21, 2022) changes to the Mastodon Trademark Policy. You will need written permission from Mastodon gGmbH to use 'Mastodon' or any derivatives (e.g. \"mastodoon\", \"mast0don\", \"mstdn\") in your domain or hostname1.</li> <li>We wanted to have our user accounts use <code>@govsocial.org</code> for all our instances instead of the actual instance hostname. As you will see later, this has implications for how both Mastodon and our load balancer are configured.</li> </ul> <p>Register your domain (we use Google Domains), set Google Cloud DNS as the nameserver, and enable DNSSEC. The console will guide you through the steps outlined here.</p> <p>If you are fine with manually creating DNS records for your platforms in Cloud DNS (which is what we do), you're done at this point.</p>"},{"location":"building/domains/#externaldns","title":"ExternalDNS","text":"<p>The The Funky Penguin's Geek Cookbook implements ExternalDNS to manage the DNS entries required for the Fediverse instances. We really tried to get this to work (and pretty much did), but in the end, it was a lot of trouble-shooting to get it working on GKE with Cloud DNS, we were unable to get it to manage everything we needed without resorting to manual entry anyway, and it's not as if your DNS entries should change often enough to make it either necessary or worthwhile.</p> <p>However, if you insist, here are some tips we learned during our efforts to get it working.</p>"},{"location":"building/domains/#service-account","title":"Service Account","text":"<p>One of the challenges about deploying in different components of a cloud service provider is granting authenticated access between these components, particularly between services running in pods on your cluster and other services such as Cloud DNS.</p> <p>The service accounts inside a GKE cluster are not part of the GCP IAM system that controls access to these other services. To grant services running GKE pods access to those services, the GKE service account in a pod needs to be mapped to an IAM account with the correct roles.</p> <p>The first thing to do is install the <code>IAM Service Account Credentials API</code> into your GCP project. You can do this by selecting the <code>IAM &amp; Admin</code> item on the GCP Console menu. If the API is not already installed, you will be prompted to add it. Once it is installed, navigate to <code>Service Accounts</code>, and create a new service account principal. You will need to assign this account the <code>DNS Administrator</code> role.</p> <p>Next, you will need to map the GKE service account in the ExternalDNS pod to this IAM service account. The instructions for doing that are here. The GKE service account for ExternalDNS will be <code>external-dns</code>, if you are following The Funky Penguin's Geek Cookbook.</p>"},{"location":"building/domains/#custom-resource-definitions-crds","title":"Custom Resource Definitions (CRDs)","text":"<p>If you are using CRDs to manage your DNS records (again, from the Cookbook), you will likely encounter this error:</p> <pre><code>error listing resources in GroupVersion \\\"externaldns.k8s.io/v1alpha1\\\": the server could not find the requested resource\n</code></pre> <p>To fix this, run this from your CLI machine:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/external-dns/master/docs/contributing/crd-source/crd-manifest.yaml\n</code></pre> <p>You will also likely encounter this error:</p> <pre><code>DNSEndpoint (\"dnsendpoints.externaldns.k8s.io is forbidden: User \\\"system:serviceaccount:external-dns:external-dns\\\" cannot list resource \\\"dnsendpoints\\\" in API group \\\"externaldns.k8s.io\\\" at the cluster scope\")\n</code></pre> <p>To this this one, run this from your CLI machine:</p> <pre><code>kubctl get clusterrole external-dns-external-dns -o yaml &gt; {file}\n</code></pre> <p>Edit the file (with <code>vi</code> or your editor of choice - not getting into that debate here!) to add the following lines at the end:</p> <pre><code>- apiGroups: [\"externaldns.k8s.io\"]\nresources: [\"dnsendpoints\"]\nverbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"externaldns.k8s.io\"]\nresources: [\"dnsendpoints/status\"]\nverbs: [\"*\"]\n</code></pre> <p>Then apply the new configuration with:</p> <pre><code>kubectl apply -f {file}\n</code></pre> <p>Are we having fun yet?</p> <p>Good, because we're not done.</p>"},{"location":"building/domains/#record-ownership-tracking","title":"Record Ownership Tracking","text":"<p>You will experience conflicts between the default <code>TXT</code> DNS records that ExternalDNS uses to track the DNS records it manages, and any actual <code>TXT</code> records that you need (for DKIM, for example). You will need to set the <code>txtSuffix:</code> value to <code>\"txt\"</code> in the external-dns overrides file to avoid this issue. <code>txtPrefix:</code> doesn't seem to work.</p> <p>Even after all this, we were unable to get two things to work at all:</p> <ul> <li>An <code>A</code> record for our \"naked domain\"</li> <li>The <code>MX</code> and <code>TXT</code> records necessary for our custom domain to work with AWS SES</li> </ul> <p>This is the point at which we gave up and reverted to manual DNS record entry. YMMV.</p> <ol> <li> <p>GOVSocial obtained this permission for our use of <code>mastodon.govsocial.org</code> on January 10, 2023\u00a0\u21a9</p> </li> </ol>"},{"location":"building/email/","title":"Transactional Email","text":"<p>Another service that most Fediverse platforms use in common is transactional email. These are the emails that allow users to confirm their email address, reset passwords, and receive the notifications they set up for themselves.</p> <p>This was the one service we were unable to host with Google, as they don't really provide a Mail Transfer Agent (MTA) that operates at scale. Personal GMail accounts do have limited mail sending capacity, but a Fediverse instance of any size will rapidly exceed that, and may have the unfortunate result of suspending your account.</p> <p>After a couple of failed attempts at persuading a couple of the well-known commercial transactional mail services of our bona fides, we settled on Amazon AWS Simple Email Service (AWS SES). It has generous daily sending limits at a very reasonable cost.</p>"},{"location":"building/email/#setting-up-aws-ses","title":"Setting up AWS SES","text":"<p>You will need to set up an AWS account, and create an SES instance. Your new SES account will be in a sandox (meaning it can only send mail to its own identities) until you apply to move out of the sandbox. This process takes about a day, so we recommend starting early, while you work on some of the other preparation steps.</p> <p>While you are waiting, take the time to verify your domain for DKIM, and set up a <code>Custom From Domain</code> (we use <code>notifications.govsocial.org</code> for ours). The console will guide you through the steps for creating the necessary DNS entries. Again, this takes a few hours to complete, so get this going early.</p> <p>Use the <code>SMTP Settings</code> menu in the SES console to create an SMTP credential in the AWS IAM. This will provide you with an account name and authentication key that you will need for your platform configuration. Save these somewhere safe - you will only be shown them once at creation time. That menu will also provide the configuration settings for the SMTP server that you will need.</p>"},{"location":"building/fluxhelm/","title":"Flux and Helm","text":"<p>One of the best decisions we made as we started our first platform install (Mastodon), was to follow the advice in The Funky Penguin's Geek Cookbook and use Flux to bootstrap our cluster and deploy Helm charts and Flux Kustomizations from a private GitHub repo. Doing this made it relatively easy to install what we needed from existing Helm charts. It also allowed us to quickly rebuild our cluster (something we did at least twice!) as we made poor configuration choices and mistakes while we figured things out.</p>"},{"location":"building/fluxhelm/#install-flux-cli","title":"Install Flux-CLI","text":"<p>To start, you will need to install the Flux CLI. Rather than install locally, we spun up an <code>e2-micro</code> VM in Google Compute Engine to host this, <code>kubectl</code>, and the other tools we would need for our build. Rather than messing with the Docker image, we installed the binaries with:</p> <pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\n</code></pre> <p>NOTE: To run <code>flux-cli</code> from a Compute Engine VM, your <code>Compute Engine default service account</code> needs the <code>Kubernetes Engine Admin</code> role. You can edit the roles assigned to service account principals in IAM. You should also get used to running this each time you connect to your VM:</p> <pre><code>gcloud container clusters get-credentials {my-cluster} --region {region}\n</code></pre> <p>This will authenticate you to your cluster.</p> <p>You should also install the <code>kubectl</code> and <code>google-cloud-sdk-gke-gcloud-auth-plugin</code> packages at this point, from your machine's package manager. Don't forget to periodically run <code>sudo apt-get update &amp;&amp; sudo apt-get upgrade</code> on your machine to keep your packages up to date - the Google Cloud packages update quite often.</p>"},{"location":"building/fluxhelm/#bootstrap-flux-on-the-cluster","title":"Bootstrap Flux on the Cluster","text":"<p>These instructions assume you are using a GitHub personal or organizational repo. Additional instructions for other git repositories can be found here.</p>"},{"location":"building/fluxhelm/#github-setup","title":"GitHub Setup","text":"<p>Before we bootstrap Flux on the cluster, a word about your GitHub repo setup. You will need a private GitHub personal or organizational repo. Make sure the repo is private, as your instance configurations, including secrets, will be stored there. When bootstrapped as detailed below, your repo will be used for all your Flux deployments, so avoid naming it for a particular cluster or platform.</p>"},{"location":"building/fluxhelm/#personal-access-token-pat","title":"Personal Access Token (PAT)","text":"<p>Flux uses a <code>classic</code> Personal Access Token (PAT) created for the personal GitHub account you use to access this repo. PATs are created and maintained in <code>&lt;&gt; Developer Settings</code> at the very bottom of the Settings menu for your account (accessed by clicking on your avatar at the top right of your GitHub window and choosing <code>Settings</code>). The PAT will need the full <code>repo</code> role. Make a note of the token (it will start with <code>gpg_</code>) - you will only be shown it once.</p> <p>By default, the PAT will expire every 30 days. You will get an email from GitHub 7 days before it expires. To rotate it:</p> <ul> <li>Delete the auth secret from the cluster with: <pre><code>kubectl -n flux-system delete secret flux-system\n</code></pre></li> <li>Rerun flux bootstrap with the same args as you use to set it up</li> <li>Flux will generate a new secret and will update the deploy key if you\u2019re using SSH deploy keys (you will be). You should get another email from GitHub telling you that a new SSH key has been created, and you're all set.</li> </ul>"},{"location":"building/fluxhelm/#bootstrap-flux","title":"Bootstrap Flux","text":"<p>Bootstrap Flux on your cluster by running the following:</p> <p><pre><code>flux bootstrap github \\\n--owner={my-github-username} \\\n--repository={my-repository} \\\n--path=clusters/{my-cluster} \\\n--personal\n</code></pre> NOTE: If you are using an organizational repo, omit <code>--personal</code> from the options.</p> <p>You will be prompted for your PAT, which you can copy from where you stored it and paste into the command prompt before hitting <code>[Enter]</code> (you won't see anything when you paste it in).</p> <p>If all goes well, you should see the following by typing <code>kubectl get pods -n flux-system</code> at the command prompt:</p> <p><pre><code>kubectl get pods -n flux-system\n</code></pre> <pre><code>helm-controller-564bf65b8b-fswms           1/1     Running     0\nkustomize-controller-7cbf46474d-5qxnw      1/1     Running     0\nnotification-controller-7b665fb8bd-t9vmr   1/1     Running     0\nsource-controller-84db8b78b9-xjkfm         1/1     Running     0\n</code></pre></p>"},{"location":"building/fluxhelm/#flux-repository-structure","title":"Flux Repository Structure","text":"<p>Having successfully carried out the above steps, you will now have started a Flux repo with the following structure plan (this will get built as we journey through the deployment of a platform):</p> <p><pre><code>/clusters\n  |/{cluster1}\n|/flux-system # &lt;-- This controls the Flux installation on the cluster\n|/[git|helm]repositories # &lt;-- This is where the Helm chart sources for the platforms are defined\n|/namespaces # &lt;-- This is where the k8s namespaces for the platforms are defined\n|/kustomizations\n      |kustomization-{platform1}.yaml #)     We create these when we deploy platforms like Mastodon\n|kustomization-{platform3}.yaml #) &lt;-- They tell flux to look in the matching directory in the\n|kustomization-{platform4}.yaml #)     repo root for instructions\n|/{cluster2}\n|/flux-system # &lt;-- This controls the Flux installation on the cluster\n|/[git|helm]repositories # &lt;-- This is where the Helm chart sources for the platforms are defined\n|/namespaces # &lt;-- This is where the k8s namespaces for the platforms are defined\n|/kustomizations\n      |kustomization-{platform2}.yaml #)     We create these when we deploy platforms like Mastodon\n|kustomization-{platform5}.yaml #) &lt;-- They tell flux to look in the matching directory in the\n|kustomization-{platform6}.yaml #)     repo root for instructions               |\n/{platform1} #)                                                                       |\n/{platform2} #)                                                                       |\n/{platform3} #) &lt;--We create these to hold configuration details for each platform &lt;--|\n/{platform4} #)\n/{platform5} #)\n/{platform6} #)\n</code></pre> This is an extremely powerful and flexible plan that allows different platforms to be deployed on multiple clusters from the same Flux repo.</p> <p>With all that set up, we can have some fun!</p>"},{"location":"building/mastodon/","title":"Building the Mastodon Instance","text":"<p>In building our Mastodon instance, we basically followed The Funky Penguin's Geek Cookbook, but with a few important changes. Some of these related to the specifics of deploying in GCP, and some related to our implementation choices for this instance.</p>"},{"location":"building/mastodon/#helm-chart-repository","title":"Helm Chart Repository","text":"<p>The first thing to do is to decide which Helm chart to use. We used the \"official\" one from the Mastodon repo. As the Cookbook points out, this isn't a Helm repository, but a regular GitHub repository with the Helm chart in it. There is also a Bitnami Helm chart, which did not exist at the time we started (it was created on December 15, 2022), and which we have not tried.</p> <p>NOTE: In all the file examples below, the path is from the root of your repo. Refer to the Flux repo plan if you're unsure where all the files go.</p> <p>Having decided which chart to use, you need to create the appropriate repository entry in your Flux repo, so Flux knows where to pull the chart from. Here is the <code>GitRepository</code> entry we use for the Mastodon repo chart (there is an example of a Bitnami Helm repo file in the Cookbook):</p> /clusters/{my-cluster}/gitrepositories/gitrepository-mastodon.yaml<pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: GitRepository\nmetadata:\nname: mastodon\nnamespace: flux-system\nspec:\ninterval: 1h0s\nref:\nbranch: main\nurl: https://github.com/mastodon/chart\n</code></pre>"},{"location":"building/mastodon/#platform-namespace","title":"Platform Namespace","text":"<p>Next, we will need to create a Kubernetes namespace for all the resources for our Mastodon instance. Namespaces create a logical boundary within your cluster, grouping related resources together:</p> /clusters/{my-cluster}/namespaces/namespace-mastodon.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\nname: mastodon\n</code></pre>"},{"location":"building/mastodon/#flux-kustomization","title":"Flux Kustomization","text":"<p>Now, we need to provide the instructions to Flux to connect to our repository (that reference was created when we bootstrapped Flux), apply our custom deployment configuration, install it into the namespace, and run some health checks to make sure it worked. This uses a Kubernetes object called a Kustomization:</p> /clusters/{my-cluster}/kustomizations/kustomization-mastodon.yaml<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta1\nkind: Kustomization\nmetadata:\nname: mastodon\nnamespace: flux-system\nspec:\ninterval: 15m\npath: ./mastodon\nprune: true # remove any elements later removed from the above path\ntimeout: 2m # if not set, this defaults to interval duration, which is 1h\nsourceRef:\nkind: GitRepository\nname: flux-system\nvalidation: server\nhealthChecks:\n- apiVersion: apps/v1\nkind: Deployment\nname: mastodon-web\nnamespace: mastodon\n- apiVersion: apps/v1\nkind: Deployment\nname: mastodon-streaming\nnamespace: mastodon\n#    - apiVersion: apps/v1\n#      kind: Deployment\n#      name: mastodon-sidekiq\n#      namespace: mastodon\n</code></pre> <p>The <code>spec.path:</code> entry refers to a <code>./mastodon</code> directory at the root of our repository. This tells Flux to look in there for the configuration information for the build (we'll create that in just a moment).</p> <p>NOTE: The Cookbook example includes a <code>healthChecks</code> entry for <code>mastodon-sidekiq</code>. This doesn't work with the Mastodon Helm chart as there is no listener in the deployed pod, so we commented it out.</p>"},{"location":"building/mastodon/#custom-configuration","title":"Custom Configuration","text":"<p>This is where the magic really happens. We have told Flux to look in the <code>./mastodon</code> directory in our GitHub respository that we bootstrapped Flux with earlier. Now, we populate that directory with the special sauce that makes the whole build work.</p>"},{"location":"building/mastodon/#configmap","title":"ConfigMap","text":"<p>A Kubernetes ConfigMap is a set of name-value pairs that acts as a sort of configuration file for the services in your running pods. Because local storage in Kubernetes pods is ephemeral (it gets detroyed and re-created when the pod redeploys), we need a permanent way to store configuration information outside the pod's local file system.</p> <p>The ConfigMap takes the place of the flat file called <code>.env.production</code> in the Mastodon directory of a server-based system that would be lost when the pod restarts in a Kubernetes-based system. The Mastodon Helm chart creates this flat file in each pod from the contents of the ConfigMap.</p> <p>A Helm chart comes with a <code>values.yaml</code> file that is the analog of a default configuration file. We want to set those default values to the ones we want, so we use our own ConfigMap to \"override\" the default values. Just like we edit a default configuration file to change the values to what we want, our ConfigMap is basically a copy of the default one with the appropriate values changed. Here is the ConfigMap for our Mastoson instance (comments have been removed for brevity and clarity):</p> /mastodon/configmap-mastodon-helm-chart-value-overrides.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: mastodon-helm-chart-value-overrides\nnamespace: mastodon\ndata:\nvalues.yaml: |-  image:\nrepository: tootsuite/mastodon\ntag: \"v4.0.2\"\npullPolicy: IfNotPresent\n\nmastodon:\ncreateAdmin:\nenabled: true\nusername: [redacted]\nemail: [redacted]\ncron:\nremoveMedia:\nenabled: true\nschedule: \"0 0 * * 0\"\nlocale: en\nlocal_domain: govsocial.org\nweb_domain: mastodon.govsocial.org\nsingleUserMode: false\nauthorizedFetch: false\npersistence:\nassets:\naccessMode: ReadWriteMany\nresources:\nrequests:\nstorage: 10Gi\nsystem:\naccessMode: ReadWriteMany\nresources:\nrequests:\nstorage: 100Gi\ns3:\nenabled: true\nforce_single_request: true\naccess_key: \"[redacted]\"\naccess_secret: \"[redacted]\"\nexistingSecret: \"\"\nbucket: \"mastodongov\"\nendpoint: \"https://storage.googleapis.com\"\nhostname: \"storage.googleapis.com\"\nregion: \"us\"\nalias_host: \"\"\nsecrets:\nsecret_key_base: \"[redacted]\"\notp_secret: \"[redacted]\"\nvapid:\nprivate_key: \"[redacted]\"\npublic_key: \"[redacted]\"\nexistingSecret: \"\"\nsidekiq:\npodSecurityContext: {}\nsecurityContext: {}\nresources: {}\naffinity: {}\nworkers:\n- name: all-queues\nconcurrency: 25\nreplicas: 1\nresources: {}\naffinity: {}\nqueues:\n- default,8\n- push,6\n- ingress,4\n- mailers,2\n- pull\n- scheduler\nsmtp:\nauth_method: plain\nca_file: /etc/ssl/certs/ca-certificates.crt\ndelivery_method: smtp\ndomain: notifications.govsocial.org\nenable_starttls: 'never'\nenable_starttls_auto: false\nfrom_address: mastodon@notifications.govsocial.org\nopenssl_verify_mode: none\nport: 465\nreply_to:\nserver: email-smtp.us-east-2.amazonaws.com\nssl: false\ntls: true\nlogin: [redacted]\npassword: [redacted]\nexistingSecret:\nstreaming:\nport: 4000\nworkers: 1\nbase_url: null\nreplicas: 1\naffinity: {}\npodSecurityContext: {}\nsecurityContext: {}\nresources: {}\nweb:\nport: 3000\nreplicas: 1\naffinity: {}\npodSecurityContext: {}\nsecurityContext: {}\nresources: {}\n\nmetrics:\nstatsd:\naddress: \"\"\n\ningress:\nenabled: false\nannotations:\ningressClassName:\nhosts:\n- host: mastodongov.org\npaths:\n- path: '/'\ntls:\n- secretName: mastodon-tls\nhosts:\n- mastodon.local\n\nelasticsearch:\nenabled: false\nimage:\ntag: 7\n\npostgresql:\nenabled: false\npostgresqlHostname: [redacted]\npostgresqlPort: 5432\nauth:\ndatabase: mastodongov\nusername: mastodon\npassword: \"[redacted]\"\npostgresPassword: \"[redacted]\"\n\nredis:\nenabled: true\nhostname: \"\"\nport: 6379\npassword: \"\"\n\nservice:\ntype: ClusterIP\nport: 80\n\nexternalAuth:\noidc:\nenabled: false\nsaml:\nenabled: false\noauth_global:\nomniauth_only: false\ncas:\nenabled: false\npam:\nenabled: false\nldap:\nenabled: false\n\npodSecurityContext:\nrunAsUser: 991\nrunAsGroup: 991\nfsGroup: 991\n\nsecurityContext: {}\n\nserviceAccount:\ncreate: true\nannotations: {}\nname: \"\"\n\npodAnnotations: {}\n\njobAnnotations: {}\n\nresources: {}\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n</code></pre> <p>The quickest way to create this file is to create this much by hand:</p> /mastodon/configmap-mastodon-helm-chart-value-overrides.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: mastodon-helm-chart-value-overrides\nnamespace: mastodon\ndata:\nvalues.yaml: |-  </code></pre> <p>and then paste in the rest of it underneath from the <code>values.yaml</code> from the Helm chart repo you used, being careful to indent everything you paste in by 4 spaces (YAML is picky about indentation).</p>"},{"location":"building/mastodon/#general","title":"General","text":"<p>You will want to set the <code>local_domain:</code> (and <code>web_domain:</code>, if it's different) values to those you configured when preparing your DNS domains.</p> <p>You will also need to pick the <code>username:</code> and <code>email:</code> for the Mastodon account that will be the initial admin user for the instance. You can add other users to various roles in the instance once it's running.</p> <p>NOTE: Our install didn't create this user - if this happens to you, there is a workaround that you can do once your instance is running.</p>"},{"location":"building/mastodon/#secrets","title":"Secrets","text":"<p>Mastodon uses a set of secrets for client/server authentication, 2FA, and authentication between its various services. Here is the relevant section of the ConfigMap:</p> <pre><code>secrets:\nsecret_key_base: \"[redacted]\"\notp_secret: \"[redacted]\"\nvapid:\nprivate_key: \"[redacted]\"\npublic_key: \"[redacted]\"\n# -- you can also specify the name of an existing Secret\n# with keys SECRET_KEY_BASE and OTP_SECRET and\n# VAPID_PRIVATE_KEY and VAPID_PUBLIC_KEY\nexistingSecret: \"\"\n</code></pre> <p>To obtain the <code>secret_key_base:</code> and <code>otp_secret:</code> values, you will need to install the <code>rake</code> package from the package manager on your CLI machine. Create a file named <code>rakefile</code> in your working directory with these contents:</p> rakefile<pre><code>desc 'Generate a cryptographically secure secret key (this is typically used to generate a secret for cookie sessions).'\ntask :secret do\nrequire 'securerandom'\nputs SecureRandom.hex(64)\nend\n</code></pre> <p>Then, run <code>rake secret</code> twice, once for the <code>secret_key_base</code>, and once for the <code>otp_secret</code>, and paste the values into your ConfigMap.</p> <p>The <code>vapid</code> key is a public/private keypair. We used an online Vapid key generator for these.</p>"},{"location":"building/mastodon/#ingress","title":"Ingress","text":"<p>Note that the <code>ingress.enabled:</code> value is set to <code>false</code>. The chart doesn't contain a spec for the GKE Ingress, and we created ours by hand once our instance was up and running.</p>"},{"location":"building/mastodon/#elastic-search","title":"Elastic Search","text":"<p>We also left <code>elasticsearch.enabled:</code> set to <code>false</code>. Elastic requires its own cluster, which would have increased our initial hosting costs considerably. We may add this feature (which allows users to perform full-text searches on their timelines) at some point.</p> <p>Now, let's walk through the specifics of how we configured Mastodon to deploy with the various services (AWS SES, Cloud Storage, and Cloud SQL) that we prepared earlier.</p>"},{"location":"building/mastodon/#smtp","title":"SMTP","text":"<p>Here is the section of our ConfigMap that relates to the AWS SES service we prepared earlier:</p> <pre><code>smtp:\nauth_method: plain\nca_file: /etc/ssl/certs/ca-certificates.crt\ndelivery_method: smtp\ndomain: notifications.govsocial.org\nenable_starttls: 'never'\nenable_starttls_auto: false\nfrom_address: mastodon@notifications.govsocial.org\nopenssl_verify_mode: none\nport: 465\nreply_to:\nserver: email-smtp.us-east-2.amazonaws.com\nssl: false\ntls: true\nlogin: [redacted]\npassword: [redacted]\n# -- you can also specify the name of an existing Secret\n# with the keys login and password\nexistingSecret:\n</code></pre> <p>This took a surprising amount of trial and error to get working. If you have problems, and you have managed to get the rest of your Mastodon instance running, the Sidekiq Retries queue (in the <code>Administration</code> settings in Mastodon) is your friend. It will provide you with useful error messages to help you troubleshoot the problem.</p> <p>The <code>domain:</code> setting will be the custom domain that you set up and verified with SES, and the <code>from_address:</code> can be any email address from that domain. You should paste the values for <code>login:</code> and <code>password:</code> from the SMTP Credential you configured in the SES console, remembering that <code>login:</code> is the randomly generated account name for the credential, not the name of the account principal itself.</p> <p>The tricky part was getting the connection to work. We could not get <code>STARTTLS</code> on port <code>587</code> to work at all, and only got implicit TLS working by setting <code>enable_starttls: 'never'</code>, <code>enable_starttls_auto: false</code>, <code>openssl_verify_mode: none</code>, <code>ssl: false</code>, and <code>tls: true</code>. </p>"},{"location":"building/mastodon/#cloud-storage","title":"Cloud Storage","text":"<p>Here is the section of our ConfigMap that relates to the S3-compatible storage we prepared earlier:</p> <pre><code>s3:\nenabled: true\nforce_single_request: true\naccess_key: \"[redacted]\"\naccess_secret: \"[redacted]\"\n# -- you can also specify the name of an existing Secret\n# with keys AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\nexistingSecret: \"\"\nbucket: \"mastodongov\"\nendpoint: \"https://storage.googleapis.com\"\nhostname: \"storage.googleapis.com\"\nregion: \"us\"\n# -- If you have a caching proxy, enter its base URL here.\nalias_host: \"\"\n</code></pre> <p>You'll paste in the <code>access_key:</code> and <code>access_secret:</code> values from the HMAC key that you created for your Cloud Storage bucket, and set <code>bucket:</code> to the name you chose for it.</p> <p>The only catch we stumbled on when getting this to work was the addition of <code>force_single_request: true</code>, as Google Cloud Storage cannot handle chunked requests.</p>"},{"location":"building/mastodon/#postgresql","title":"PostgreSQL","text":"<p>Here is the section of our ConfigMap that relates to the PostgreSQL database we prepared earlier:</p> <pre><code>postgresql:\n# -- disable if you want to use an existing db; in which case the values below\n# must match those of that external postgres instance\nenabled: false\npostgresqlHostname: [redacted]\npostgresqlPort: 5432\nauth:\ndatabase: mastodongov\nusername: mastodon\n# you must set a password; the password generated by the postgresql chart will\n# be rotated on each upgrade:\n# https://github.com/bitnami/charts/tree/master/bitnami/postgresql#upgrade\npassword: \"[redacted]\"\n# Set the password for the \"postgres\" admin user\n# set this to the same value as above if you've previously installed\n# this chart and you're having problems getting mastodon to connect to the DB\npostgresPassword: \"[redacted]\"\n# you can also specify the name of an existing Secret\n# with a key of password set to the password you want\n# existingSecret: \"\"\n</code></pre> <p>The first thing to note is that <code>postgres.enabled:</code> is set to <code>false</code>. This seems counter-intuitive - after all, we need a PostgreSQL database for the thing to work. In this case, the <code>enabled:</code> setting really tells the Mastodon deployment whether or not to enable a database locally in the cluster or not. In our deployment, we did not want that - we wanted to use the Cloud SQL database that we already created.</p> <p>The <code>postgresqlHostname:</code> setting will be the internal IP address of your PostgreSQL instance.</p> <p>Remember that we created a separate database for each platform we are running, so we changed the <code>database:</code> and <code>username:</code> to match what we created. Because we are also using a different user for the platform database, we needed to set both the <code>password:</code> (which is the password of the account in the <code>username:</code> setting) and the <code>postgresPassword:</code> (which is the password of the default <code>postgres</code> account) to the correct values. Mastodon uses each for different database tasks, so it needs both passwords in this configuration.</p> <p>When you get to the actual deployment and your pods spin up, you should notice a Job called <code>mastodon-db-migrate</code> spin up as well. This job is creating the correct database schema for your instance. Your other Mastodon pods may not be available until that job completes.</p> <p>NOTE: You may find that the <code>mastodon-db-migrate</code> job doesn't run with <code>postgres.enabled</code> set to <code>false</code> (although our experience was based on incorrectly setting it to <code>true</code> in our initial deployment and desperately trying to switch to Cloud SQL after deploying Mastodon1 ).</p> <p>YMMV with a correctly configured fresh install, but if that happens to you, here is how we fixed it. The <code>mastodon-web</code> and <code>mastodon-sidekiq</code> pods will fail to start, but the <code>mastodon-streaming</code> pod will because, unlike the other two, it is not dependent on a database connection. The dirty little secret is that all three pods are running the same Mastodon image, so we can use the running <code>mastodon-streaming</code> pod to access a running Mastodon service and the <code>rails</code> environment we need.</p> <p>Connect to the running <code>mastodon-streaming</code> pod from your CLI machine by entering this:</p> <p><pre><code>kubectl get pods -n mastodon\n</code></pre> <pre><code>mastodon-streaming-bb578b4bc-gzfr2            1/1     Running     0\nmastodon-streaming-bb578b4bc-nszjf            1/1     Running     0\nmastodon-streaming-bb578b4bc-wsv2l            1/1     Running     0\n</code></pre> <pre><code>kubectl exec -it mastodon-streaming-bb578b4bc-wsv2l -n mastodon /bin/sh\n</code></pre></p> <p>It doesn't matter which pod you connect to, if there is more than one, like in the example above. Once you are connected to a pod, run the following:</p> <pre><code>$ export OTP_SECRET=[redacted]\n$ export SECRET_KEY_BASE=[redacted]\n$ RAILS_ENV=production bundle exec rails db:migrate\n</code></pre> <p>You should see the <code>mastodon-db-migrate</code> job appear in your <code>Workloads</code> in the Cloud Console, and this will prepare your database. Once the job is finished, your <code>mastodon-web</code> and <code>mastodon-sidekiq</code> pods should restart successfully.</p>"},{"location":"building/mastodon/#helmrelease","title":"HelmRelease","text":"<p>Now that we have our instance-specific ConfigMap in place, we can create the HelmRelease that will pull the Helm chart from the repository we chose for it and apply our ConfigMap to it. Here is the HelmRelease file we use:</p> /mastodon/helmrelease-mastodon.yaml<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\nname: mastodon\nnamespace: mastodon\nspec:\nchart:\nspec:\nchart: ./\nsourceRef:\nkind: GitRepository\nname: mastodon\nnamespace: flux-system\ninterval: 15m\ntimeout: 5m\nreleaseName: mastodon\nvaluesFrom:\n- kind: ConfigMap\nname: mastodon-helm-chart-value-overrides\nvaluesKey: values.yaml </code></pre> <p>With all this in place, here is the sequence of events:</p> <ul> <li>The <code>/clusters/{my-cluster}/kustomizations/kustomization-mastodon.yaml</code> Kustomization tells Flux on that cluster to monitor the <code>./mastodon</code> path our repository (<code>name: flux-system</code>) for changes every <code>interval: 15m</code></li> <li>When a change is detected, it updates the <code>name: mastodon-helm-chart-value-overrides</code> ConfigMap from our repo on the cluster, and...</li> <li>pulls the <code>name: mastodon</code> HelmRelease, which points at the platform repository (<code>name: mastodon</code>) containing the Helm chart that builds the pods and services deploys the chart in our cluster, with their local <code>.env.production</code> files built from the updated ConfigMap.</li> <li>The <code>healthChecks</code> in the Kustomization are run to make sure everything deployed successfully.</li> </ul> <p>NOTE: The platform repository is itself monitored for changes every <code>interval: 15m</code>, and changes in that will also trigger a Flux reconcilliation. If you want to avoid unexpected upgrades, you can specify a valid <code>image.tag</code> in your ConfigMap. This is particularly important now that v4.1 is imminent, and the published Helm Chart could change without warning.</p>"},{"location":"building/mastodon/#deploy-mastodon","title":"Deploy Mastodon","text":"<p>You can either wait for Flux to detect your changes, or you can speed up the process by running the following from your CLI machine:</p> <pre><code>flux reconcile source git flux-system\n</code></pre> <p>You can see the Mastodon pods by running the following from your CLI machine (or looking in your GKE console):</p> <p><pre><code>kubectl get pods -n mastodon\n</code></pre> <pre><code>mastodon-redis-master-0                       1/1     Running\nmastodon-redis-replicas-0                     1/1     Running\nmastodon-redis-replicas-1                     1/1     Running\nmastodon-redis-replicas-2                     1/1     Running\nmastodon-sidekiq-all-queues-7d8b8c596-456kx   1/1     Running\nmastodon-sidekiq-all-queues-7d8b8c596-97rfv   1/1     Running\nmastodon-sidekiq-all-queues-7d8b8c596-98d2x   1/1     Running\nmastodon-streaming-5bdc55888b-95fbn           1/1     Running\nmastodon-streaming-5bdc55888b-cwnsv           1/1     Running\nmastodon-streaming-5bdc55888b-czh28           1/1     Running\nmastodon-web-56cc95dd99-k7mfg                 1/1     Running\nmastodon-web-56cc95dd99-n524q                 1/1     Running\nmastodon-web-56cc95dd99-vmkxf                 1/1     Running\n</code></pre></p> <p>That's it! You're done deploying your Mastodon instance on GKE! Now, we need to make sure people can access it.</p>"},{"location":"building/mastodon/#ingress_1","title":"Ingress","text":"<p>You will remember that we did not enable the ingress that is included in the Mastodon Helm chart and instead opted to configure the GKE Ingress by hand.</p> <p>You can do this in the console by going to <code>Services &amp; Ingress</code> in the GKE menu in Google Cloud Console. You will need an <code>External HTTPS Ingress</code> with two ingress paths to make Mastodon work properly, especially with mobile applications:</p> <ul> <li>A path for the <code>mastodon-streaming</code> service, with the path set to <code>/api/v1/streaming</code></li> <li>A path for the <code>mastodon-web</code> service, with the path set to <code>/*</code></li> </ul> <p>As part of creating an HTTPS ingress, you will need a TLS certificate. We opted to use a Google-manged certificate. The domain for the certificate needs to be for the <code>web_domain</code> of the instance (or <code>local_domain</code>, if <code>web_domain</code> is not set).</p> <p>The <code>spec</code> for the resulting ingress should look like this:</p> <pre><code>spec:\nrules:\n- http:\npaths:\n- backend:\nservice:\nname: mastodon-web\nport:\nnumber: 3000\npath: /*\npathType: ImplementationSpecific\n- backend:\nservice:\nname: mastodon-streaming\nport:\nnumber: 4000\npath: /api/v1/streaming\npathType: ImplementationSpecific\n</code></pre> <p>The Ingress will be allocated an external IP address that you should add as an <code>A</code> record for the instance hostname in your DNS record set. Your TLS certificate will not validate until that DNS record propogates (usually within half an hour or so).</p> <p>Once it's all up and running, you should be able to connect to your instance from your web browser!</p>"},{"location":"building/mastodon/#load-balancer","title":"Load Balancer","text":"<p>For GOVSocial.org, we wanted our user accounts to have the scheme <code>{username}@govsocial.org</code>, rather than having the full domain of each platform in them, like <code>{username}@{platform}.govsocial.org</code>. This means that our <code>local_domain</code> in Mastodon is <code>govsocial.org</code>, while our <code>web_domain</code> is <code>mastodon.govsocial.org</code>.</p> <p>This poses challenges for federation, as any links to user profiles on other instances will intially connect to <code>govsocial.org</code> (the <code>local_domain</code>) instead of our <code>web_domain</code> and will need to be redirected. This redirection is handled by a <code>webfinger</code> service in Mastodon.</p> <p>The load balancer needs to redirect requests for <code>govsocial.org/.well-known/webfinger</code> (which is where other instances think it is based on our username scheme) to <code>mastodon.govsocial.org/.well-known/webfinger</code> (where the service is actually listening).</p> <p>To do this, we deployed an <code>HTTPS (classic) Load Balancer</code> in the <code>Network Services -&gt; Load Balancing</code> menu in our Google Cloud Console. The setup is very similar to the Ingress we set up earlier. In fact, you will see the load balancer created by the Ingress in the list when you go there.</p> <p>HINT: Don't mess with it :-) If you're the sort of person who can't help pressing big red buttons that say \"DO NOT PRESS\", it's okay - anything you change will eventually be reverted by the GKE configuration, but your ingress might be broken until that happens.</p> <p>Create a new <code>HTTPS (classic) Load Balancer</code> (this one supports the hostname and path redirect features we need). Despite the higher cost, we selected the Premium network tier, as it allows for the addition of services like Cloud Armor and Cloud CDN if the platform needs it in the future.</p> <p>For the Frontend configuration, make sure to create a new fixed external IP address and add the corresponding <code>A</code> record for <code>local_domain</code> in your DNS record set. Because we want to use HTTPS, you will need to create a TLS certificate for your <code>local_domain</code>. The certificate won't validate until this DNS record propogates (usually with half an hour or so).</p> <p>For the Backend configuration, pick the default <code>kube-system-default-http-backend-80</code> service (there will be a bunch of letters/numbers before and after it.) This service doesn't have anything listening on it, but it will be used for the mandatory default rule in the rules configuration.</p> <p>In the <code>Host and path rules</code>, create a <code>Prefix redirect</code> for your <code>local_domain</code>, set the <code>Host redirect</code> to your <code>web_domain</code>, and the <code>Paths</code> to <code>/.well-known/webfinger</code>. Select <code>301 - Moved Permanently</code> for the response, and make sure that <code>HTTPS redirect</code> is enabled.</p> <p>Save your configuration, and wait for it to become available in the Cloud Console.</p> <p>NOTE: One of the advantages of having this load balancer in conjunction with our domain scheme is that it means that we can use the default path (<code>/*</code>) of GOVSocial.org for documentation and non-instance specific content. We created a similar rule in our load balancer for <code>/*</code> that redirects to <code>docs.govsocial.org</code>, which is what you are reading now. There is a whole other write-up for that!</p> <ol> <li> <p>Flux to the rescue again! This was one of the issues (the other was abandoning AutoPilot) that had us delete all the Mastodon workloads and start over. The postgreSQL configuration seems to be particularly \"sticky\" and, try as we might, we could not get the corrected configuration to take after the initial deployment.\u00a0\u21a9</p> </li> </ol>"},{"location":"building/postgresql/","title":"PostgreSQL","text":"<p>As with S3-compatible storage, most Fediverse platforms require a PostgreSQL database instance where much of the instance data is stored. Despite the importance of the performance of the database to that of the overall instance, you can save a lot of up-front cost by starting small and cloning to a larger machine as you grow.</p> <p>We use a Google CloudSQL PostgreSQL instance, selecting the smallest possible machine (<code>db-f1-micro</code>) and set the storage to <code>auto-grow</code>.</p> <p>You will need to ensure that you enable the Private IP interface for the instance so the GKE cluster can reach it. We did not enable the CloudSQL Auth Proxy - we may eventually do that at some point.</p> <p>We chose to create a separate database and user for each platform, rather than piling everything into the default <code>postgres</code> one.</p>"},{"location":"building/storage/","title":"S3 Storage","text":"<p>Most Fediverse platforms require some form of S3-compatible storage for images and other media files.</p> <p>We use Google Cloud Storage for this. Your bucket will need to be <code>public</code>, with <code>fine-grained</code> access control, and use the <code>standard</code> storage class.</p> <p>The service account principal doesn't need any roles - you'll use a HMAC key for access. Store this key safely - you will only be shown it once at creation time.</p>"},{"location":"operating/","title":"Operating GOVSocial","text":"<p>Building our Fediverse instance was one thing, but we also learned a lot about operating them as well, especially in the \"safe for work\" mode and the culture of openness and transparency that are fundamental values of the public service. We are public servants ourselves, and embrace the principle of \"Default to Open\" from the US Digital Service Playbook.</p> <p>This created three clear goals for the GOVSocial project:</p> <ol> <li>To provide a comprehensive set of documentation detailing exactly how the platforms were built, so individual and agency users alike could see behind our technological, operational, and financial curtains.</li> <li>To provide assurance that account registration on our instances is, to the best of our ability and within the constraints of the deployed technology, accurately tied to transparent identification and validation of public service users and agencies.</li> <li>To provide, to the best of our ability and within the constraints of the deployed technology, a comprehensive and transparent user and server moderation policy, to shield our users from association with federated and local content that could harm the reputation of themselves or their agencies.</li> </ol> <p>Each of these goals presented some technical and operation challenges. In this section, we have documented the approaches and techniques we adopted to meet them.</p>"}]}