{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to GOVSocial!","text":"<p>GovSocial's mission is to provide instances of Fediverse social media platforms \"safe\" for use by public service employees in Federal, State, tribal, and local government, public education, and other publicly-funded organizations.</p> <p>As public servants ourselves, we believe in the US Digital Service Playbook principle of \"Default to Open\", so this site provides links to how we build and run our instances, and transparent reporting on the operating performance and costs of our instances.</p>"},{"location":"#our-platforms","title":"Our Platforms","text":"<ul> <li> <p>Mastodon</p> <p></p> </li> </ul>"},{"location":"#what-to-expect-when-you-join","title":"What to expect when you join","text":"<p>Our instance federation and moderation polices are stricter than many general use instances.</p> <p>It is a requirement that you sign up with your work email address to verify your identity as a public service employee. .gov email addresses are automatically approved, while others will be manually approved, based on cross-checking the domain part of the email address with that of the organization's public website, and confirming that the organization is a public service organization.</p> <p>You must also use your real name, and the use of official headshots or logos for account avatars is strongly encouraged. You are also encouraged to use your profile to provide links (verified, if possible) back to the appropriate pages on your organization's website, your LinkedIn profile, and any others that provide confidence to other users that you are who you say you are.</p> <p>All content must be appropriate for your work environment, and in keeping with your presence on our instances as a verified public service employee. You may also want to check with your organization's social media policy before registering.</p> <p>Organizational accounts and bots are encouraged, provided they meet the same requirements as individual accounts, and follow the instance rules.</p> <p>We hope you enjoy your time here. Please reach out to the instance team if you have any questions or concerns. Have fun!</p>"},{"location":"building/","title":"Building GOVSocial","text":"<p>When we first began thinking of hosting Fediverse platforms for public service use, one of our first considerations was whether we would self-host or host in the cloud, and whether we wanted to deploy on servers or in a containerized environment.</p> <p>We wanted to start small, and scale quickly as needed without service interruption. We started the project in November 2022, just as the first wave of refugees from commercial platforms started, and were keenly aware of the struggles of existing instances to rapidly scale to meet demand and to control the escalating hosting costs of a large influx of users.</p> <p>This need for scalable reliability and control over our hosting costs led us to pick a Kubernetes deployment hosted in a major cloud service provider. This met the US Digital Service Playbook principles of \"Choose a modern technology stack\" and \"Deploy in a flexible hosting environment\".</p> <p>We picked Google primarily because they are committed to open standards and have a proven track record of support for the public sector.</p> <p>We set out to prove that we could implement our entire platform using GCP services. As you will see, we managed that with the sole exception of transactional email, as Google does not currently provide such a service, at least at any sort of scale.</p> <p>The project is indebted to two excellent guides on hosting our first instance, Mastodon, on Kubernetes and on Google Cloud Platform (GCP):</p> <ul> <li>The Funky Penguin's Geek Cookbook</li> <li>Justin Ribeiro's guide to installing Mastodon on GCP</li> </ul> <p>We made quite a few necessary modifications to both of these, and learned a ton along the way. We have documented our journey here to assist others interested in a similar implementation.</p>"},{"location":"building/cluster/","title":"Creating the cluster","text":"<p>None of the how-tos we found covered the creation of the initial Kubernetes cluster, because this is really down to the choice of hosting environment and appetite for cost. We created a standard cluster in GKE with the following cost-saving caveats:</p> <ul> <li>As tempting as it sounds from an ease of management perspective, do NOT create an AutoPilot cluster. The minimum vCPU and Memory requests for an AutoPilot cluster will bankrupt you before you start (roughly $200/month!).</li> <li>Use spot VMs for the nodes on which your cluster will be deployed. They offer significant cost savings over standard VMs and allow you to use decent-sized machines for your nodes at a fraction of the cost. We chose the e2-medium shared core machines and set the cluster to auto-scale from 0 to 3 nodes. You will want to make sure to set a Pod Disruption Budget (PDB) to make sure that auto-updating and pre-emption of the Spot VM nodes doesn't disrupt your instance services. We took the advice given here to set <code>maxSurge=1 maxUnavailable=0</code>.</li> <li>Start with the smallest cluster available. You can always add more and beefier nodes to it later.</li> </ul> <p>Important</p> <p>You will need to make sure you create your cluster with Workload Identity enabled. Authenticated access from the cluster service account (which is NOT an IAM service account) to other GCP services such as Cloud Storage depends on it.</p>"},{"location":"building/domains/","title":"DNS Domains","text":"<p>You will need a public DNS domain for your Fediverse instances if you want them to be accessible from the Internet. Because we wanted to host more than one Fediverse platform under the GOVSocial umbrella, we choose <code>govsocial.org</code> as our root domain, with <code>{instance}.govsocial.org</code> as the instance-specific hostname.</p> <p>For Mastodon, there are a couple of things to be aware of when choosing a domain in general, and this configuration in particular:</p> <ul> <li>Be aware of recent (December 21, 2022) changes to the Mastodon Trademark Policy. You will need written permission from Mastodon gGmbH to use 'Mastodon' or any derivatives (e.g. \"mastodoon\", \"mast0don\", \"mstdn\") in your domain or hostname1.</li> <li>We wanted to have our user accounts use <code>@govsocial.org</code> for all our instances instead of the actual instance hostname. As you will see later, this has implications for how both Mastodon and our load balancer are configured.</li> </ul> <p>Register your domain (we use Google Domains), set Google Cloud DNS as the nameserver, and enable DNSSEC. The console will guide you through the steps outlined here.</p> <p>If you are fine with manually creating DNS records for your platforms in Cloud DNS (which is what we do), you're done at this point.</p>"},{"location":"building/domains/#externaldns","title":"ExternalDNS","text":"<p>The The Funky Penguin's Geek Cookbook implements ExternalDNS to manage the DNS entries required for the Fediverse instances. We really tried to get this to work (and pretty much did), but in the end, it was a lot of trouble-shooting to get it working on GKE with Cloud DNS, we were unable to get it to manage everything we needed without resorting to manual entry anyway, and it's not as if your DNS entries should change often enough to make it either necessary or worthwhile.</p> <p>However, if you insist, here are some tips we learned during our efforts to get it working.</p>"},{"location":"building/domains/#service-account","title":"Service Account","text":"<p>One of the challenges about deploying in different components of a cloud service provider is granting authenticated access between these components, particularly between services running in pods on your cluster and other services such as Cloud DNS.</p> <p>The service accounts inside a GKE cluster are not part of the GCP IAM system that controls access to these other services. To grant services running GKE pods access to those services, the GKE service account in a pod needs to be mapped to an IAM account with the correct roles.</p> <p>The first thing to do is install the <code>IAM Service Account Credentials API</code> into your GCP project. You can do this by selecting the <code>IAM &amp; Admin</code> item on the GCP Console menu. If the API is not already installed, you will be prompted to add it. Once it is installed, navigate to <code>Service Accounts</code>, and create a new service account principal. You will need to assign this account the <code>DNS Administrator</code> role.</p> <p>Next, you will need to map the GKE service account in the ExternalDNS pod to this IAM service account. The instructions for doing that are here. The GKE service account for ExternalDNS will be <code>external-dns</code>, if you are following The Funky Penguin's Geek Cookbook.</p>"},{"location":"building/domains/#custom-resource-definitions-crds","title":"Custom Resource Definitions (CRDs)","text":"<p>If you are using CRDs to manage your DNS records (again, from the Cookbook), you will likely encounter this error:</p> <pre><code>error listing resources in GroupVersion \\\"externaldns.k8s.io/v1alpha1\\\": the server could not find the requested resource\n</code></pre> <p>To fix this, run this from your CLI machine:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/external-dns/master/docs/contributing/crd-source/crd-manifest.yaml\n</code></pre> <p>You will also likely encounter this error:</p> <pre><code>DNSEndpoint (\"dnsendpoints.externaldns.k8s.io is forbidden: User \\\"system:serviceaccount:external-dns:external-dns\\\" cannot list resource \\\"dnsendpoints\\\" in API group \\\"externaldns.k8s.io\\\" at the cluster scope\")\n</code></pre> <p>To this this one, run this from your CLI machine:</p> <pre><code>kubctl get clusterrole external-dns-external-dns -o yaml &gt; {file}\n</code></pre> <p>Edit the file (with <code>vi</code> or your editor of choice - not getting into that debate here!) to add the following lines at the end:</p> <pre><code>- apiGroups: [\"externaldns.k8s.io\"]\nresources: [\"dnsendpoints\"]\nverbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"externaldns.k8s.io\"]\nresources: [\"dnsendpoints/status\"]\nverbs: [\"*\"]\n</code></pre> <p>Then apply the new configuration with:</p> <pre><code>kubectl apply -f {file}\n</code></pre> <p>Are we having fun yet?</p> <p>Good, because we're not done.</p>"},{"location":"building/domains/#record-ownership-tracking","title":"Record Ownership Tracking","text":"<p>You will experience conflicts between the default <code>TXT</code> DNS records that ExternalDNS uses to track the DNS records it manages, and any actual <code>TXT</code> records that you need (for DKIM, for example). You will need to set the <code>txtSuffix:</code> value to <code>\"txt\"</code> in the external-dns overrides file to avoid this issue. <code>txtPrefix:</code> doesn't seem to work.</p> <p>Even after all this, we were unable to get two things to work at all:</p> <ul> <li>An <code>A</code> record for our \"naked domain\"</li> <li>The <code>MX</code> and <code>TXT</code> records necessary for our custom domain to work with AWS SES</li> </ul> <p>This is the point at which we gave up and reverted to manual DNS record entry. YMMV.</p> <ol> <li> <p>GOVSocial obtained this permission for our use of <code>mastodon.govsocial.org</code> on January 10, 2023\u00a0\u21a9</p> </li> </ol>"},{"location":"building/email/","title":"Transactional Email","text":"<p>Another service that most Fediverse platforms use in common is transactional email. These are the emails that allow users to confirm their email address, reset passwords, and receive the notifications they set up for themselves.</p> <p>This was the one service we were unable to host with Google, as they don't really provide a Mail Transfer Agent (MTA) that operates at scale. Personal GMail accounts do have limited mail sending capacity, but a Fediverse instance of any size will rapidly exceed that, and may have the unfortunate result of suspending your account.</p> <p>After a couple of failed attempts at persuading a couple of the well-known commercial transactional mail services of our bona fides, we settled on Amazon AWS Simple Email Service (AWS SES). It has generous daily sending limits at a very reasonable cost.</p>"},{"location":"building/email/#setting-up-aws-ses","title":"Setting up AWS SES","text":"<p>You will need to set up an AWS account, and create an SES instance. Your new SES account will be in a sandox (meaning it can only send mail to its own identities) until you apply to move out of the sandbox. This process takes about a day, so we recommend starting early, while you work on some of the other preparation steps.</p> <p>While you are waiting, take the time to verify your domain for DKIM, and set up a <code>Custom From Domain</code> (we use <code>notifications.govsocial.org</code> for ours). The console will guide you through the steps for creating the necessary DNS entries. Again, this takes a few hours to complete, so get this going early.</p> <p>Use the <code>SMTP Settings</code> menu in the SES console to create an SMTP credential in the AWS IAM. This will provide you with an account name and authentication key that you will need for your platform configuration.</p> <p>Warning</p> <p>Save these somewhere safe - you will only be shown them once at creation time.</p> <p>That menu will also provide the configuration settings for the SMTP server that you will need.</p>"},{"location":"building/fediblockhole/","title":"Building a k8s FediBlockHole Cron Job","text":"<p>FediBlockHole is a Python tool that is designed to be installed on a machine and run from the command line. We could have installed it on our CLI VM and run it from there with a crontab entry to automate it, but there are a couple of disadvantages with this approach. Firstly, it would require our VM to be more stable than a spot VM, increasing our hosting costs. Secondly, the supported VM image in GCP comes with Python 3.9.x, and FediBlockHole requires at least version 3.10.</p> <p>Rather than embarking on that expedition, we decided to containerize FediBlockHole, and deploy a Kubernetes cron job that instantiates the container and runs the script.</p> <p>Note</p> <p>When we did this, we forked rather than cloning the FediBlockHole repo into our own repo. We wanted to submit a pull request for ingrating our work back into the original project, and to keep our repo up to date with it. The following documentation is from that perspective. All repo paths are relative to the root of our forked repo.</p>"},{"location":"building/fediblockhole/#containerizing-fediblockhole","title":"Containerizing FediBlockHole","text":"<p>To prepare for containerizing FediBlockHole, we installed Docker on our CLI machine from its package manager:</p> <pre><code>~$ sudo apt-get install docker.io\n</code></pre> <p>Note</p> <p>In a GCP Compute VM, the package is called <code>docker.io</code>. YMMV.</p> <p>Next, we prepared a Dockerfile for the Docker container:</p> /container/Dockerfile<pre><code># Use the official lightweight Python image.\n# https://hub.docker.com/_/python\nFROM python:slim\n\n# Copy local code to the container image.\nENV APP_HOME /app\nWORKDIR $APP_HOME\n\n# Install production dependencies.\nRUN pip install fediblockhole\n\nUSER 1001\n# Run the script on container startup.\nENTRYPOINT [\"fediblock-sync\"]\n</code></pre> <p>We also made sure there was a Python-flavored Docker ignore file:</p> /container/.dockerignore<pre><code>Dockerfile\n#README.md\n*.pyc\n*.pyo\n*.pyd\n__pycache__\n</code></pre> <p>Then, we built our Docker image:</p> <p><pre><code>~$ sudo docker build https://github.com/[user|organization]/fediblockhole.git#main:container --no-cache\n</code></pre> <pre><code>Successfully built {container-id}\n</code></pre></p> <p>Note</p> <p>The <code>--no-cache</code> option forces a refresh from the repo rather than using the local cache. This is important for updates.</p> <p>Using the <code>{container-id}</code> from <code>docker build</code>, we annotated the container with the version of FediBlockHole it was built with (currently <code>0.4.2</code>), and <code>latest</code>:</p> <pre><code>~$ sudo docker image tag {container-id} fediblockhole:latest\n~$ sudo docker image tag {container-id} fediblockhole:0.4.2\n~$ sudo docker image tag fediblockhole:latest ghcr.io/[user|organization]/fediblockhole:latest\n~$ sudo docker image tag fediblockhole:0.4.2 ghcr.io/[user|organization]/fediblockhole:0.4.2\n</code></pre> <p>We signed into our GitHub repository with this command:</p> <pre><code>~$ sudo docker login ghcr.io -u {user_name}\n</code></pre> <p>Once signed in, we pushed the image to our <code>Packages</code> in GitHub:</p> <pre><code>~$ sudo docker push -a ghcr.io/[user|organization]/fediblockhole\n</code></pre> <p>The completed package can be found here.</p>"},{"location":"building/fediblockhole/#creating-the-helm-chart","title":"Creating the Helm Chart","text":"<p>Next, we wanted to leverage the Flux/Helm system we built earlier to deploy a Kubernetes cron job in our cluster.</p>"},{"location":"building/fediblockhole/#chart-file","title":"Chart File","text":"<p>The first thing we needed was a Helm chart:</p> /chart/Chart.yaml<pre><code>apiVersion: v2\nname: fediblockhole\ndescription: FediBlockHole is a tool for keeping a Mastodon instance blocklist synchronised with remote lists.\n\n# A chart can be either an 'application' or a 'library' chart.\n#\n# Application charts are a collection of templates that can be packaged into versioned archives\n# to be deployed.\n#\n# Library charts provide useful utilities or functions for the chart developer. They're included as\n# a dependency of application charts to inject those utilities and functions into the rendering\n# pipeline. Library charts do not define any templates and therefore cannot be deployed.\ntype: application\n\n# This is the chart version. This version number should be incremented each time you make changes\n# to the chart and its templates, including the app version.\n# Versions are expected to follow Semantic Versioning (https://semver.org/)\nversion: 1.0.0\n\n# This is the version number of the application being deployed. This version number should be\n# incremented each time you make changes to the application. Versions are not expected to\n# follow Semantic Versioning. They should reflect the version the application is using.\nappVersion: 0.4.2\n</code></pre>"},{"location":"building/fediblockhole/#helm-ignore-file","title":"Helm Ignore File","text":"<p>We also created a Helm ignore file:</p> /chart/.helmignore<pre><code># A helm chart's templates and default values can be packaged into a .tgz file.\n# When doing that, not everything should be bundled into the .tgz file. This\n# file describes what to not bundle.\n#\n# Manually added by us\n# --------------------\n#\n\n# Boilerplate .helmignore from `helm create mastodon`\n# ---------------------------------------------------\n#\n# Patterns to ignore when building packages.\n# This supports shell glob matching, relative path matching, and\n# negation (prefixed with !). Only one pattern per line.\n.DS_Store\n# Common VCS dirs\n.git/\n.gitignore\n.bzr/\n.bzrignore\n.hg/\n.hgignore\n.svn/\n# Common backup files\n*.swp\n*.bak\n*.tmp\n*.orig\n*~\n# Various IDEs\n.project\n.idea/\n*.tmproj\n.vscode/\n</code></pre>"},{"location":"building/fediblockhole/#values-file","title":"Values File","text":"<p>Every Helm chart needs a Values file, which passes values into the chart's templates for the build. This is the Values file for this chart:</p> /chart/values.yaml<pre><code>image:\nrepository: ghcr.io/cunningpike/fediblockhole\n# https://github.com/cunningpike/fediblockhole/pkgs/container/fediblockhole/versions\n#\n# alternatively, use `latest` for the latest release or `edge` for the image\n# built from the most recent commit\n#\n# tag: latest\ntag: \"\"\n# use `Always` when using `latest` tag\npullPolicy: IfNotPresent\n\nfediblockhole:\n# location of the configuration file. Default is /etc/default/fediblockhole.conf.toml\nconf_file:\npath: \"\"\nfilename: \"\"\ncron:\n# -- run `fediblock-sync` every hour\nsync:\n# @ignored\nenabled: false\n# @ignored\nschedule: \"0 * * * *\"\n\n# if you manually change the UID/GID environment variables, ensure these values\n# match:\npodSecurityContext:\nrunAsUser: 991\nrunAsGroup: 991\nfsGroup: 991\n\n# @ignored\nsecurityContext: {}\n\n# -- Kubernetes manages pods for jobs and pods for deployments differently, so you might\n# need to apply different annotations to the two different sets of pods. The annotations\n# set with podAnnotations will be added to all deployment-managed pods.\npodAnnotations: {}\n\n# -- The annotations set with jobAnnotations will be added to all job pods.\njobAnnotations: {}\n\n# -- Default resources for all Deployments and jobs unless overwritten\nresources: {}\n# We usually recommend not to specify default resources and to leave this as a conscious\n# choice for the user. This also increases chances charts run on environments with little\n# resources, such as Minikube. If you do want to specify resources, uncomment the following\n# lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n# limits:\n#   cpu: 100m\n#   memory: 128Mi\n# requests:\n#   cpu: 100m\n#   memory: 128Mi\n\n# @ignored\nnodeSelector: {}\n\n# @ignored\ntolerations: []\n\n# -- Affinity for all pods unless overwritten\naffinity: {}\n</code></pre> <p>The <code>repository:</code> value points to the location of the GitHub Package we pushed our Docker image to. The <code>tag:</code> value can be used to override the <code>appVersion:</code> value in the chart.</p> <p>Note</p> <p>We specified a non-root user and group in our Values file to make sure that the containerized script does not run as the root user.</p>"},{"location":"building/fediblockhole/#templates","title":"Templates","text":"<p>All the preceding files are really just metadata for the Helm deployment - the real work is done in the templates.</p>"},{"location":"building/fediblockhole/#cron-job","title":"Cron Job","text":"<p>The template for the Kubernetes cron job we needed looks like this:</p> /chart/templates/cronjob-fediblock-sync.yaml<pre><code>{{ if .Values.fediblockhole.cron.sync.enabled -}}\napiVersion: batch/v1\nkind: CronJob\nmetadata:\nname: {{ include \"fediblockhole.fullname\" . }}-sync\nlabels:\n{{- include \"fediblockhole.labels\" . | nindent 4 }}\nspec:\nschedule: {{ .Values.fediblockhole.cron.sync.schedule }}\njobTemplate:\nspec:\ntemplate:\nmetadata:\nname: {{ include \"fediblockhole.fullname\" . }}-sync\n{{- with .Values.jobAnnotations }}\nannotations:\n{{- toYaml . | nindent 12 }}\n{{- end }}\nspec:\nrestartPolicy: OnFailure\ncontainers:\n- name: {{ include \"fediblockhole.fullname\" . }}-sync\nimage: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"\nimagePullPolicy: {{ .Values.image.pullPolicy }}\ncommand:\n- fediblock-sync\n- -c\n- \"{{- include \"fediblockhole.conf_file_path\" . -}}{{- include \"fediblockhole.conf_file_filename\" . -}}\"\nvolumeMounts:\n- name: config\nmountPath: {{ include \"fediblockhole.conf_file_path\" . | quote }}\nvolumes:\n- name: config\nconfigMap:\nname: {{ include \"fediblockhole.fullname\" . }}-conf-toml\nitems:\n- key: {{ include \"fediblockhole.conf_file_filename\" . | quote }}\npath: {{ include \"fediblockhole.conf_file_filename\" . | quote }}\n{{- end }}\n</code></pre> <p>You can see the values from the Values file (prefixed with <code>.Values.</code>) in the template. An example of where a value from the Values file overrides the default from the Chart is highlighted in the file above.</p>"},{"location":"building/fediblockhole/#helpers-file","title":"Helpers File","text":"<p>There are several values in the template (prefixed with <code>fediblockhole.</code>) that are not defined in either the Values file or the Chart. These are defined in a Helpers file, like this:</p> /chart/templates/_helpers.tpl<pre><code>{{/* vim: set filetype=mustache: */}}\n{{/*\nExpand the name of the chart.\n*/}}\n{{- define \"fediblockhole.name\" -}}\n{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n\n{{/*\nCreate a default fully qualified app name.\nWe truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).\nIf release name contains chart name it will be used as a full name.\n*/}}\n{{- define \"fediblockhole.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- if contains $name .Release.Name }}\n{{- .Release.Name | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n{{- end }}\n\n{{/*\nCreate chart name and version as used by the chart label.\n*/}}\n{{- define \"fediblockhole.chart\" -}}\n{{- printf \"%s-%s\" .Chart.Name .Chart.Version | replace \"+\" \"_\" | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n\n{{/*\nCommon labels\n*/}}\n{{- define \"fediblockhole.labels\" -}}\nhelm.sh/chart: {{ include \"fediblockhole.chart\" . }}\n{{ include \"fediblockhole.selectorLabels\" . }}\n{{- if .Chart.AppVersion }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n{{- end }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n\n{{/*\nSelector labels\n*/}}\n{{- define \"fediblockhole.selectorLabels\" -}}\napp.kubernetes.io/name: {{ include \"fediblockhole.name\" . }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end }}\n\n{{/*\nRolling pod annotations\n*/}}\n{{- define \"fediblockhole.rollingPodAnnotations\" -}}\nrollme: {{ .Release.Revision | quote }}\nchecksum/config-configmap: {{ include ( print $.Template.BasePath \"/configmap-conf-toml.yaml\" ) . | sha256sum | quote }}\n{{- end }}\n\n{{/*\nCreate the default conf file path and filename\n*/}}\n{{- define \"fediblockhole.conf_file_path\" -}}\n{{- default \"/etc/default/\" .Values.fediblockhole.conf_file.path }}\n{{- end }}\n{{- define \"fediblockhole.conf_file_filename\" -}}\n{{- default \"fediblockhole.conf.toml\" .Values.fediblockhole.conf_file.filename }}\n{{- end }}\n</code></pre> <p>You can see how values from both the Chart file and the Values file are combined to define other values that are then used in the cron job template.</p> <p>A slightly different syntax for defining defaults and overrides is highlighted above, showing how the local path and filename for the FediBlockHole configuration has a default value of <code>/etc/default/fediblockhole.conf.toml</code> from the Chart file if a value is not set in the Values file.</p>"},{"location":"building/fediblockhole/#configuration-file","title":"Configuration File","text":"<p>As pointed out at the start, FediBlockHole was intended to be deployed on a machine with a persistant storage system, and read a TOML-formatted configuration file from a local filepath. This poses a challenge when containerizing it, as we need to be able to easily make changes to the configuration file and redeploy the cron job with the updated values into the pod's local file system where the script expects to find it. We also want to make sure that the container is rebuilt with the new configuration file when the changes are made to it.</p> <p>There are a few tricks in the various files that we used to accomplish this. First, we created a ConfigMap template, which globs a copy of the default configuration file stored in the root of the chart into a ConfigMap entry:</p> /chart/templates/configmap-conf-toml.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: {{ include \"fediblockhole.fullname\" . }}-conf-toml\nlabels:\n{{- include \"fediblockhole.labels\" . | nindent 4 }}\ndata:\n{{ (.Files.Glob \"fediblockhole.conf.toml\").AsConfig | nindent 4 }}\n</code></pre> <p>Next, we included in the container spec a local filesystem mount that mounts the configuration file into the expected location in the container's local filesystem:</p> /chart/templates/cronjob-fediblock-sync.yaml<pre><code>...\nspec:\n...\ncontainers:\n...\nvolumeMounts:\n- name: config\nmountPath: {{ include \"fediblockhole.conf_file_path\" . | quote }}\nvolumes:\n- name: config\nconfigMap:\nname: {{ include \"fediblockhole.fullname\" . }}-conf-toml\nitems:\n- key: {{ include \"fediblockhole.conf_file_filename\" . | quote }}\npath: {{ include \"fediblockhole.conf_file_filename\" . | quote }}\n</code></pre> <p>Finally, we set the container to restart when a change to the ConfigMap is detected by annotating it with the checksum of the ConfigMap:</p> /chart/templates/_helpers.tpl<pre><code>...\n{{/*\nRolling pod annotations\n*/}}\n{{- define \"fediblockhole.rollingPodAnnotations\" -}}\nrollme: {{ .Release.Revision | quote }}\nchecksum/config-configmap: {{ include ( print $.Template.BasePath \"/configmap-conf-toml.yaml\" ) . | sha256sum | quote }}\n{{- end }}\n...\n</code></pre> <p>All this ensures that a new container image is generated with the latest configuration file in the correct location in its local filesystem, each time a change is made to the ConfigMap.</p> <p>Note</p> <p>Remember that references to the ConfigMap in the chart are relative to the repository from which the Helm Release will actually be run. This will be our Flux/Helm repository that we prepared earlier, allowing us to specify our own ConfigMap containing our version of the configuration file.</p>"},{"location":"building/fediblockhole/#oauth-token","title":"OAuth Token","text":"<p>In order to push blocks to your Mastodon instance, you will need a Mastodon OAuth token with <code>admin:read</code> and <code>admin:write</code> scopes. You will paste the value from the <code>Your access token</code> field in the Mastodon <code>Development</code> screen into this block of the FediBlockHole configuration file as highlighted here:</p> <pre><code># List of instances to write blocklist to\nblocklist_instance_destinations = [\n{ domain = '{my-web_domain}', token = '[redacted]', max_followed_severity = 'silence'},\n]\n</code></pre> <p>You won't need to set the <code>Redirect URI</code> to anything other than the default - it's just the token that FediBlockHole needs.</p> <p>Danger, Will Robinson</p> <p>Protect your FediBlockHole configuration file carefully once this token is placed in it, and rotate the token regularly. If compromised, it is all anyone will need to have full admin access to your instance via the API.</p>"},{"location":"building/fediblockhole/#deploying-fediblockhole","title":"Deploying FediBlockHole","text":"<p>Deploying our FediBlockHole cron job into our cluster followed an identical pattern to how we deployed Mastodon. We prepared our deployment by creating a GitHub repository source and namespace for our cron job:</p> /clusters/{my-cluster}/gitrepositories/gitrepository-fediblockhole.yaml<pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: GitRepository\nmetadata:\nname: fediblockhole\nnamespace: flux-system\nspec:\ninterval: 5m\nref:\nbranch: main\nurl: https://github.com/cunningpike/fediblockhole/\n</code></pre> /clusters/{my-cluster}/namespaces/namespace-fediblockhole.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\nname: fediblockhole\n</code></pre> <p>Next, we created the Flux kustomization:</p> /clusters/{my-cluster}/kustomizations/kustomization-fediblockhole.yaml<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta1\nkind: Kustomization\nmetadata:\nname: fediblockhole\nnamespace: flux-system\nspec:\ninterval: 5m\npath: ./fediblockhole\nprune: true # remove any elements later removed from the above path\ntimeout: 2m # if not set, this defaults to interval duration, which is 1h\nsourceRef:\nkind: GitRepository\nname: flux-system\nvalidation: server\n</code></pre> <p>Then, we created our own version of the ConfigMap that configures the FediBlockHole cron job itself:</p> /fediblockhole/configmap-fediblockhole-helm-chart-value-overrides.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: fediblockhole-helm-chart-value-overrides\nnamespace: fediblockhole\ndata:\nvalues.yaml: |-  image:\nrepository: ghcr.io/cunningpike/fediblockhole\n# https://github.com/cunningpike/fediblockhole/pkgs/container/fediblockhole/versions\n#\n# alternatively, use `latest` for the latest release or `edge` for the image\n# built from the most recent commit\n#\n# tag: latest\ntag: \"\"\n# use `Always` when using `latest` tag\npullPolicy: IfNotPresent\n\nfediblockhole:\n# location of the configuration file. Default is /etc/default/fediblockhole.conf.toml\nconf_file:\npath: \"\"\nfilename: \"\"\ncron:\n# -- run `fediblock-sync` every hour\nsync:\n# @ignored\nenabled: true\n# @ignored\nschedule: \"0 */2 * * *\"\n\n# if you manually change the UID/GID environment variables, ensure these values\n# match:\npodSecurityContext:\nrunAsUser: 991\nrunAsGroup: 991\nfsGroup: 991\n\n# @ignored\nsecurityContext: {}\n\n# -- Kubernetes manages pods for jobs and pods for deployments differently, so you might\n# need to apply different annotations to the two different sets of pods. The annotations\n# set with podAnnotations will be added to all deployment-managed pods.\npodAnnotations: {}\n\n# -- The annotations set with jobAnnotations will be added to all job pods.\njobAnnotations: {}\n\n# -- Default resources for all Deployments and jobs unless overwritten\nresources: {}\n# We usually recommend not to specify default resources and to leave this as a conscious\n# choice for the user. This also increases chances charts run on environments with little\n# resources, such as Minikube. If you do want to specify resources, uncomment the following\n# lines, adjust them as necessary, and remove the curly braces after 'resources:'.\n# limits:\n#   cpu: 100m\n#   memory: 128Mi\n# requests:\n#   cpu: 100m\n#   memory: 128Mi\n\n# @ignored\nnodeSelector: {}\n\n# @ignored\ntolerations: []\n\n# -- Affinity for all pods unless overwritten\naffinity: {}\n</code></pre> <p>The only changes you need to make in this file are:</p> <ul> <li>Set <code>fediblockhole.cron.sync.enabled:</code> to <code>true</code>.</li> <li>Set the desired crontab value in <code>fediblockhole.cron.sync.schedule:</code>. We started with <code>\"0 */2 * * *\"</code> initially, as shown, but have since reduced that to <code>\"0 * * * *\"</code> as updates are much faster after the initial sync.</li> </ul> <p>After that, we set up our version of the FediBlockHole configuration file:</p> /fediblockhole/configmap-fediblockhole-conf-toml.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: fediblockhole-conf-toml\nnamespace: fediblockhole\ndata:\nfediblockhole.conf.toml: |-  # List of instances to read blocklists from.\n# If the instance makes its blocklist public, no authorization token is needed.\n#   Otherwise, `token` is a Bearer token authorised to read domain_blocks.\n# If `admin` = True, use the more detailed admin API, which requires a token with a \n#   higher level of authorization.\n# If `import_fields` are provided, only import these fields from the instance.\n#   Overrides the global `import_fields` setting.\nblocklist_instance_sources = [\n# { domain = 'public.blocklist'}, # an instance with a public list of domain_blocks\n# { domain = 'jorts.horse', token = '&lt;a_different_token&gt;' }, # user accessible block list\n# { domain = 'eigenmagic.net', token = '&lt;a_token_with_read_auth&gt;', admin = true }, # admin access required\n]\n\n# List of URLs to read csv blocklists from\n# Format tells the parser which format to use when parsing the blocklist\n# max_severity tells the parser to override any severities that are higher than this value\n# import_fields tells the parser to only import that set of fields from a specific source\nblocklist_url_sources = [\n# { url = 'file:///path/to/fediblockhole/samples/demo-blocklist-01.csv', format = 'csv' },\n{ url = 'https://codeberg.org/oliphant/blocklists/raw/branch/main/blocklists/_unified_min_blocklist.csv', format = 'csv' },\n{ url = 'https://rapidblock.org/blocklist.json', format = 'rapidblock.json' },\n]\n\n## These global allowlists override blocks from blocklists\n# These are the same format and structure as blocklists, but they take precedence\nallowlist_url_sources = [\n# { url = 'https://raw.githubusercontent.com/eigenmagic/fediblockhole/main/samples/demo-allowlist-01.csv', format = 'csv' },\n{ url = 'https://codeberg.org/oliphant/blocklists/raw/branch/main/blocklists/__allowlist.csv', format = 'csv' },\n]\n\n# List of instances to write blocklist to\nblocklist_instance_destinations = [\n{ domain = 'mastodon.govsocial.org', token = '[redacted]', max_followed_severity = 'silence'},\n]\n\n## Store a local copy of the remote blocklists after we fetch them\n#save_intermediate = true\n\n## Directory to store the local blocklist copies\n# savedir = '/tmp'\n\n## File to save the fully merged blocklist into\n# blocklist_savefile = '/tmp/merged_blocklist.csv'\n\n## Don't push blocklist to instances, even if they're defined above\n# no_push_instance = false\n\n## Don't fetch blocklists from URLs, even if they're defined above\n# no_fetch_url = false\n\n## Don't fetch blocklists from instances, even if they're defined above\n# no_fetch_instance = false\n\n## Set the mergeplan to use when dealing with overlaps between blocklists\n# The default 'max' mergeplan will use the harshest severity block found for a domain.\n# The 'min' mergeplan will use the lightest severity block found for a domain.\n# mergeplan = 'max'\n\n## Set which fields we import\n## 'domain' and 'severity' are always imported, these are additional\n## \nimport_fields = ['public_comment', 'reject_media', 'reject_reports', 'obfuscate']\n\n## Set which fields we export\n## 'domain' and 'severity' are always exported, these are additional\n## \nexport_fields = ['public_comment']\n</code></pre> <p>More information about how we chose our block list sources can be found here.</p> <p>Finally, we created the Helm Release that deployed the configured cron job into our cluster:</p> /fediblockhole/helmrelease-fediblockhole.yaml<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\nname: fediblockhole\nnamespace: fediblockhole\nspec:\nchart:\nspec:\nchart: ./chart\nsourceRef:\nkind: GitRepository\nname: fediblockhole\nnamespace: flux-system\ninterval: 15m\ntimeout: 5m\nreleaseName: fediblockhole\nvaluesFrom:\n- kind: ConfigMap\nname: fediblockhole-helm-chart-value-overrides\nvaluesKey: values.yaml\n</code></pre> <p>A Couple of Notes</p> <ul> <li>Rate-limiting (either in our Ingress or in the Mastodon API itself) makes running FediBlockHole on our instance a relatively slow operation, at least for an initial load. YMMV, but we recommend setting your <code>fediblockhole.cron.sync.schedule:</code> to a fairly lengthy interval (at least 2 hours) until you get a feel for how long a typical run takes in your environment.</li> <li>FediBlockHole currently does NOT delete expired blocks i.e. if an existing block in your instance is no longer in a pulled list, it needs to be removed manually. We are planning to contribute code to the FediBlockProject to implement this feature.</li> <li>v1.0.0 of our Helm chart for FediBlockHole does not include ConfigMaps for the local file import functionality. This will be included in the next release.</li> </ul>"},{"location":"building/fluxhelm/","title":"Flux and Helm","text":"<p>One of the best decisions we made as we started our first platform install (Mastodon), was to follow the advice in The Funky Penguin's Geek Cookbook and use Flux to bootstrap our cluster and deploy Helm charts and Flux Kustomizations from a private GitHub repo. Doing this made it relatively easy to install what we needed from existing Helm charts. It also allowed us to quickly rebuild our cluster (something we did at least twice!) as we made poor configuration choices and mistakes while we figured things out.</p>"},{"location":"building/fluxhelm/#install-flux-cli","title":"Install Flux-CLI","text":"<p>To start, you will need to install the Flux CLI. Rather than install locally, we spun up an <code>e2-micro</code> VM in Google Compute Engine to host this, <code>kubectl</code>, and the other tools we would need for our build. Rather than messing with the Docker image, we installed the binaries with:</p> <pre><code>curl -s https://fluxcd.io/install.sh | sudo bash\n</code></pre> <p>Note</p> <p>To run <code>flux-cli</code> from a Compute Engine VM, your <code>Compute Engine default service account</code> needs the <code>Kubernetes Engine Admin</code> role. You can edit the roles assigned to service account principals in IAM.</p> <p>You should also get used to running this each time you connect to your VM:</p> <pre><code>gcloud container clusters get-credentials {my-cluster} --region {region}\n</code></pre> <p>This will authenticate you to your cluster.</p> <p>You should also install the <code>kubectl</code> and <code>google-cloud-sdk-gke-gcloud-auth-plugin</code> packages at this point, from your machine's package manager. Don't forget to periodically run <code>sudo apt-get update &amp;&amp; sudo apt-get upgrade</code> on your machine to keep your packages up to date - the Google Cloud packages update quite often.</p>"},{"location":"building/fluxhelm/#bootstrap-flux-on-the-cluster","title":"Bootstrap Flux on the Cluster","text":"<p>These instructions assume you are using a GitHub personal or organizational repo. Additional instructions for other git repositories can be found here.</p>"},{"location":"building/fluxhelm/#github-setup","title":"GitHub Setup","text":"<p>Before we bootstrap Flux on the cluster, a word about your GitHub repo setup. You will need a private GitHub personal or organizational repo.</p> <p>Danger, Will Robinson</p> <p>Make sure the repo is private, as your instance configurations, including secrets, will be stored there.</p> <p>When bootstrapped as detailed below, your repo will be used for all your Flux deployments, so avoid naming it for a particular cluster or platform.</p>"},{"location":"building/fluxhelm/#personal-access-token-pat","title":"Personal Access Token (PAT)","text":"<p>Flux uses a <code>classic</code> Personal Access Token (PAT) created for the personal GitHub account you use to access this repo. PATs are created and maintained in <code>&lt;&gt; Developer Settings</code> at the very bottom of the Settings menu for your account (accessed by clicking on your avatar at the top right of your GitHub window and choosing <code>Settings</code>). The PAT will need the full <code>repo</code> role.</p> <p>Warning</p> <p>Make a note of the token (it will start with <code>gpg_</code>) - you will only be shown it once.</p> <p>By default, the PAT will expire every 30 days. You will get an email from GitHub 7 days before it expires. To rotate it:</p> <ul> <li>Delete the auth secret from the cluster with: <pre><code>kubectl -n flux-system delete secret flux-system\n</code></pre></li> <li>Rerun flux bootstrap with the same args as you use to set it up</li> <li>Flux will generate a new secret and will update the deploy key if you\u2019re using SSH deploy keys (you will be). You should get another email from GitHub telling you that a new SSH key has been created, and you're all set.</li> </ul>"},{"location":"building/fluxhelm/#bootstrap-flux","title":"Bootstrap Flux","text":"<p>Bootstrap Flux on your cluster by running the following:</p> <pre><code>flux bootstrap github \\\n--owner={my-github-username} \\\n--repository={my-repository} \\\n--path=clusters/{my-cluster} \\\n--personal\n</code></pre> <p>Note</p> <p>If you are using an organizational repo, omit <code>--personal</code> from the options.</p> <p>You will be prompted for your PAT, which you can copy from where you stored it and paste into the command prompt before hitting <code>[Enter]</code> (you won't see anything when you paste it in).</p> <p>If all goes well, you should see the following by typing <code>kubectl get pods -n flux-system</code> at the command prompt:</p> <p><pre><code>kubectl get pods -n flux-system\n</code></pre> <pre><code>helm-controller-564bf65b8b-fswms           1/1     Running     0\nkustomize-controller-7cbf46474d-5qxnw      1/1     Running     0\nnotification-controller-7b665fb8bd-t9vmr   1/1     Running     0\nsource-controller-84db8b78b9-xjkfm         1/1     Running     0\n</code></pre></p>"},{"location":"building/fluxhelm/#flux-repository-structure","title":"Flux Repository Structure","text":"<p>Having successfully carried out the above steps, you will now have started a Flux repo with the following structure plan (this will get built as we journey through the deployment of a platform):</p> <p><pre><code>/clusters\n  |/{cluster1}\n|/flux-system # &lt;-- This controls the Flux installation on the cluster\n|/[git|helm]repositories # &lt;-- This is where the Helm chart sources for the platforms are defined\n|/namespaces # &lt;-- This is where the k8s namespaces for the platforms are defined\n|/kustomizations\n      |kustomization-{platform1}.yaml #)     We create these when we deploy platforms like Mastodon\n|kustomization-{platform3}.yaml #) &lt;-- They tell flux to look in the matching directory in the\n|kustomization-{platform4}.yaml #)     repo root for instructions\n|/{cluster2}\n|/flux-system # &lt;-- This controls the Flux installation on the cluster\n|/[git|helm]repositories # &lt;-- This is where the Helm chart sources for the platforms are defined\n|/namespaces # &lt;-- This is where the k8s namespaces for the platforms are defined\n|/kustomizations\n      |kustomization-{platform2}.yaml #)     We create these when we deploy platforms like Mastodon\n|kustomization-{platform5}.yaml #) &lt;-- They tell flux to look in the matching directory in the\n|kustomization-{platform6}.yaml #)     repo root for instructions               |\n/{platform1} #)                                                                       |\n/{platform2} #)                                                                       |\n/{platform3} #) &lt;--We create these to hold configuration details for each platform &lt;--|\n/{platform4} #)\n/{platform5} #)\n/{platform6} #)\n</code></pre> This is an extremely powerful and flexible plan that allows different platforms to be deployed on multiple clusters from the same Flux repo.</p> <p>With all that set up, we can have some fun!</p>"},{"location":"building/mastodon/","title":"Building the Mastodon Instance","text":"<p>In building our Mastodon instance, we basically followed The Funky Penguin's Geek Cookbook, but with a few important changes. Some of these related to the specifics of deploying in GCP, and some related to our implementation choices for this instance.</p>"},{"location":"building/mastodon/#helm-chart-repository","title":"Helm Chart Repository","text":"<p>The first thing to do is to decide which Helm chart to use. We used the \"official\" one from the Mastodon repo. As the Cookbook points out, this isn't a Helm repository, but a regular GitHub repository with the Helm chart in it. There is also a Bitnami Helm chart, which did not exist at the time we started (it was created on December 15, 2022), and which we have not tried.</p> <p>Note</p> <p>In all the file examples below, the path is from the root of your repo. Refer to the Flux repo plan if you're unsure where all the files go.</p> <p>Having decided which chart to use, you need to create the appropriate repository entry in your Flux repo, so Flux knows where to pull the chart from. Here is the <code>GitRepository</code> entry we use for the Mastodon repo chart (there is an example of a Bitnami Helm repo file in the Cookbook):</p> /clusters/{my-cluster}/gitrepositories/gitrepository-mastodon.yaml<pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: GitRepository\nmetadata:\nname: mastodon\nnamespace: flux-system\nspec:\ninterval: 1h0s\nref:\nbranch: main\nurl: https://github.com/mastodon/chart\n</code></pre>"},{"location":"building/mastodon/#platform-namespace","title":"Platform Namespace","text":"<p>Next, we will need to create a Kubernetes namespace for all the resources for our Mastodon instance. Namespaces create a logical boundary within your cluster, grouping related resources together:</p> /clusters/{my-cluster}/namespaces/namespace-mastodon.yaml<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\nname: mastodon\n</code></pre>"},{"location":"building/mastodon/#flux-kustomization","title":"Flux Kustomization","text":"<p>Now, we need to provide the instructions to Flux to connect to our repository (that reference was created when we bootstrapped Flux), apply our custom deployment configuration, install it into the namespace, and run some health checks to make sure it worked. This uses a Kubernetes object called a Kustomization:</p> /clusters/{my-cluster}/kustomizations/kustomization-mastodon.yaml<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1beta1\nkind: Kustomization\nmetadata:\nname: mastodon\nnamespace: flux-system\nspec:\ninterval: 15m\npath: ./mastodon\nprune: true # remove any elements later removed from the above path\ntimeout: 2m # if not set, this defaults to interval duration, which is 1h\nsourceRef:\nkind: GitRepository\nname: flux-system\nvalidation: server\nhealthChecks:\n- apiVersion: apps/v1\nkind: Deployment\nname: mastodon-web\nnamespace: mastodon\n- apiVersion: apps/v1\nkind: Deployment\nname: mastodon-streaming\nnamespace: mastodon\n#    - apiVersion: apps/v1\n#      kind: Deployment\n#      name: mastodon-sidekiq\n#      namespace: mastodon\n</code></pre> <p>The <code>spec.path:</code> entry refers to a <code>./mastodon</code> directory at the root of our repository. This tells Flux to look in there for the configuration information for the build (we'll create that in just a moment).</p> <p>Note</p> <p>The Cookbook example includes a <code>healthChecks</code> entry for <code>mastodon-sidekiq</code>. This doesn't work with the Mastodon Helm chart as there is no listener in the deployed pod, so we commented it out.</p>"},{"location":"building/mastodon/#custom-configuration","title":"Custom Configuration","text":"<p>This is where the magic really happens. We have told Flux to look in the <code>./mastodon</code> directory in our GitHub respository that we bootstrapped Flux with earlier. Now, we populate that directory with the special sauce that makes the whole build work.</p>"},{"location":"building/mastodon/#configmap","title":"ConfigMap","text":"<p>A Kubernetes ConfigMap is a set of name-value pairs that acts as a sort of configuration file for the services in your running pods. Because local storage in Kubernetes pods is ephemeral (it gets detroyed and re-created when the pod redeploys), we need a permanent way to store configuration information outside the pod's local file system.</p> <p>The ConfigMap takes the place of the flat file called <code>.env.production</code> in the Mastodon directory of a server-based system that would be lost when the pod restarts in a Kubernetes-based system. The Mastodon Helm chart creates this flat file in each pod from the contents of the ConfigMap.</p> <p>A Helm chart comes with a <code>values.yaml</code> file that is the analog of a default configuration file. We want to set those default values to the ones we want, so we use our own ConfigMap to \"override\" the default values. Just like we edit a default configuration file to change the values to what we want, our ConfigMap is basically a copy of the default one with the appropriate values changed. Here is the ConfigMap for our Mastoson instance (comments have been removed for brevity and clarity):</p> configmap-mastodon-helm-chart-value-overrides.yaml <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: mastodon-helm-chart-value-overrides\nnamespace: mastodon\ndata:\nvalues.yaml: |-  image:\nrepository: tootsuite/mastodon\ntag: \"v4.0.2\"\npullPolicy: IfNotPresent\n\nmastodon:\ncreateAdmin:\nenabled: true\nusername: [redacted]\nemail: [redacted]\ncron:\nremoveMedia:\nenabled: true\nschedule: \"0 0 * * 0\"\nlocale: en\nlocal_domain: govsocial.org\nweb_domain: mastodon.govsocial.org\nsingleUserMode: false\nauthorizedFetch: false\npersistence:\nassets:\naccessMode: ReadWriteMany\nresources:\nrequests:\nstorage: 10Gi\nsystem:\naccessMode: ReadWriteMany\nresources:\nrequests:\nstorage: 100Gi\ns3:\nenabled: true\nforce_single_request: true\naccess_key: \"[redacted]\"\naccess_secret: \"[redacted]\"\nexistingSecret: \"\"\nbucket: \"mastodongov\"\nendpoint: \"https://storage.googleapis.com\"\nhostname: \"storage.googleapis.com\"\nregion: \"us\"\nalias_host: \"\"\nsecrets:\nsecret_key_base: \"[redacted]\"\notp_secret: \"[redacted]\"\nvapid:\nprivate_key: \"[redacted]\"\npublic_key: \"[redacted]\"\nexistingSecret: \"\"\nsidekiq:\npodSecurityContext: {}\nsecurityContext: {}\nresources: {}\naffinity: {}\nworkers:\n- name: all-queues\nconcurrency: 25\nreplicas: 1\nresources: {}\naffinity: {}\nqueues:\n- default,8\n- push,6\n- ingress,4\n- mailers,2\n- pull\n- scheduler\nsmtp:\nauth_method: plain\nca_file: /etc/ssl/certs/ca-certificates.crt\ndelivery_method: smtp\ndomain: notifications.govsocial.org\nenable_starttls: 'never'\nenable_starttls_auto: false\nfrom_address: mastodon@notifications.govsocial.org\nopenssl_verify_mode: none\nport: 465\nreply_to:\nserver: email-smtp.us-east-2.amazonaws.com\nssl: false\ntls: true\nlogin: [redacted]\npassword: [redacted]\nexistingSecret:\nstreaming:\nport: 4000\nworkers: 1\nbase_url: null\nreplicas: 1\naffinity: {}\npodSecurityContext: {}\nsecurityContext: {}\nresources: {}\nweb:\nport: 3000\nreplicas: 1\naffinity: {}\npodSecurityContext: {}\nsecurityContext: {}\nresources: {}\n\nmetrics:\nstatsd:\naddress: \"\"\n\ningress:\nenabled: false\nannotations:\ningressClassName:\nhosts:\n- host: mastodongov.org\npaths:\n- path: '/'\ntls:\n- secretName: mastodon-tls\nhosts:\n- mastodon.local\n\nelasticsearch:\nenabled: false\nimage:\ntag: 7\n\npostgresql:\nenabled: false\npostgresqlHostname: [redacted]\npostgresqlPort: 5432\nauth:\ndatabase: mastodongov\nusername: mastodon\npassword: \"[redacted]\"\npostgresPassword: \"[redacted]\"\n\nredis:\nenabled: true\nhostname: \"\"\nport: 6379\npassword: \"\"\n\nservice:\ntype: ClusterIP\nport: 80\n\nexternalAuth:\noidc:\nenabled: false\nsaml:\nenabled: false\noauth_global:\nomniauth_only: false\ncas:\nenabled: false\npam:\nenabled: false\nldap:\nenabled: false\n\npodSecurityContext:\nrunAsUser: 991\nrunAsGroup: 991\nfsGroup: 991\n\nsecurityContext: {}\n\nserviceAccount:\ncreate: true\nannotations: {}\nname: \"\"\n\npodAnnotations: {}\n\njobAnnotations: {}\n\nresources: {}\n\nnodeSelector: {}\n\ntolerations: []\n\naffinity: {}\n</code></pre> <p>The quickest way to create this file is to create this much by hand:</p> /mastodon/configmap-mastodon-helm-chart-value-overrides.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: mastodon-helm-chart-value-overrides\nnamespace: mastodon\ndata:\nvalues.yaml: |-  </code></pre> <p>and then paste in the rest of it underneath from the <code>values.yaml</code> from the Helm chart repo you used, being careful to indent everything you paste in by 4 spaces (YAML is picky about indentation).</p>"},{"location":"building/mastodon/#general","title":"General","text":"<p>You will want to set the <code>local_domain:</code> (and <code>web_domain:</code>, if it's different) values to those you configured when preparing your DNS domains.</p> <p>You will also need to pick the <code>username:</code> and <code>email:</code> for the Mastodon account that will be the initial admin user for the instance. You can add other users to various roles in the instance once it's running.</p> <p>Note</p> <p>Our install didn't create this user - if this happens to you, there is a workaround that you can do once your instance is running.</p>"},{"location":"building/mastodon/#secrets","title":"Secrets","text":"<p>Mastodon uses a set of secrets for client/server authentication, 2FA, and authentication between its various services. Here is the relevant section of the ConfigMap:</p> <pre><code>secrets:\nsecret_key_base: \"[redacted]\"\notp_secret: \"[redacted]\"\nvapid:\nprivate_key: \"[redacted]\"\npublic_key: \"[redacted]\"\n# -- you can also specify the name of an existing Secret\n# with keys SECRET_KEY_BASE and OTP_SECRET and\n# VAPID_PRIVATE_KEY and VAPID_PUBLIC_KEY\nexistingSecret: \"\"\n</code></pre> <p>To obtain the <code>secret_key_base:</code> and <code>otp_secret:</code> values, you will need to install the <code>rake</code> package from the package manager on your CLI machine. Create a file named <code>rakefile</code> in your working directory with these contents:</p> rakefile<pre><code>desc 'Generate a cryptographically secure secret key (this is typically used to generate a secret for cookie sessions).'\ntask :secret do\nrequire 'securerandom'\nputs SecureRandom.hex(64)\nend\n</code></pre> <p>Then, run <code>rake secret</code> twice, once for the <code>secret_key_base</code>, and once for the <code>otp_secret</code>, and paste the values into your ConfigMap.</p> <p>The <code>vapid</code> key is a public/private keypair. We used an online Vapid key generator for these.</p>"},{"location":"building/mastodon/#ingress","title":"Ingress","text":"<p>Note that the <code>ingress.enabled:</code> value is set to <code>false</code>. The chart doesn't contain a spec for the GKE Ingress, and we created ours by hand once our instance was up and running.</p>"},{"location":"building/mastodon/#elastic-search","title":"Elastic Search","text":"<p>We also left <code>elasticsearch.enabled:</code> set to <code>false</code>. Elastic requires its own cluster, which would have increased our initial hosting costs considerably. We may add this feature (which allows users to perform full-text searches on their timelines) at some point.</p> <p>Now, let's walk through the specifics of how we configured Mastodon to deploy with the various services (AWS SES, Cloud Storage, and Cloud SQL) that we prepared earlier.</p>"},{"location":"building/mastodon/#smtp","title":"SMTP","text":"<p>Here is the section of our ConfigMap that relates to the AWS SES service we prepared earlier:</p> <pre><code>smtp:\nauth_method: plain\nca_file: /etc/ssl/certs/ca-certificates.crt\ndelivery_method: smtp\ndomain: notifications.govsocial.org\nenable_starttls: 'never'\nenable_starttls_auto: false\nfrom_address: mastodon@notifications.govsocial.org\nopenssl_verify_mode: none\nport: 465\nreply_to:\nserver: email-smtp.us-east-2.amazonaws.com\nssl: false\ntls: true\nlogin: [redacted]\npassword: [redacted]\n# -- you can also specify the name of an existing Secret\n# with the keys login and password\nexistingSecret:\n</code></pre> <p>This took a surprising amount of trial and error to get working. If you have problems, and you have managed to get the rest of your Mastodon instance running, the Sidekiq Retries queue (in the <code>Administration</code> settings in Mastodon) is your friend. It will provide you with useful error messages to help you troubleshoot the problem.</p> <p>The <code>domain:</code> setting will be the custom domain that you set up and verified with SES, and the <code>from_address:</code> can be any email address from that domain. You should paste the values for <code>login:</code> and <code>password:</code> from the SMTP Credential you configured in the SES console, remembering that <code>login:</code> is the randomly generated account name for the credential, not the name of the account principal itself.</p> <p>The tricky part was getting the connection to work. We could not get <code>STARTTLS</code> on port <code>587</code> to work at all, and only got implicit TLS working by setting <code>enable_starttls: 'never'</code>, <code>enable_starttls_auto: false</code>, <code>openssl_verify_mode: none</code>, <code>ssl: false</code>, and <code>tls: true</code>. </p>"},{"location":"building/mastodon/#cloud-storage","title":"Cloud Storage","text":"<p>Here is the section of our ConfigMap that relates to the S3-compatible storage we prepared earlier:</p> <pre><code>s3:\nenabled: true\nforce_single_request: true\naccess_key: \"[redacted]\"\naccess_secret: \"[redacted]\"\n# -- you can also specify the name of an existing Secret\n# with keys AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\nexistingSecret: \"\"\nbucket: \"mastodongov\"\nendpoint: \"https://storage.googleapis.com\"\nhostname: \"storage.googleapis.com\"\nregion: \"us\"\n# -- If you have a caching proxy, enter its base URL here.\nalias_host: \"\"\n</code></pre> <p>You'll paste in the <code>access_key:</code> and <code>access_secret:</code> values from the HMAC key that you created for your Cloud Storage bucket, and set <code>bucket:</code> to the name you chose for it.</p> <p>The only catch we stumbled on when getting this to work was the addition of <code>force_single_request: true</code>, as Google Cloud Storage cannot handle chunked requests.</p>"},{"location":"building/mastodon/#postgresql","title":"PostgreSQL","text":"<p>Here is the section of our ConfigMap that relates to the PostgreSQL database we prepared earlier:</p> <pre><code>postgresql:\n# -- disable if you want to use an existing db; in which case the values below\n# must match those of that external postgres instance\nenabled: false\npostgresqlHostname: [redacted]\npostgresqlPort: 5432\nauth:\ndatabase: mastodongov\nusername: mastodon\n# you must set a password; the password generated by the postgresql chart will\n# be rotated on each upgrade:\n# https://github.com/bitnami/charts/tree/master/bitnami/postgresql#upgrade\npassword: \"[redacted]\"\n# Set the password for the \"postgres\" admin user\n# set this to the same value as above if you've previously installed\n# this chart and you're having problems getting mastodon to connect to the DB\npostgresPassword: \"[redacted]\"\n# you can also specify the name of an existing Secret\n# with a key of password set to the password you want\n# existingSecret: \"\"\n</code></pre> <p>The first thing to note is that <code>postgres.enabled:</code> is set to <code>false</code>. This seems counter-intuitive - after all, we need a PostgreSQL database for the thing to work. In this case, the <code>enabled:</code> setting really tells the Mastodon deployment whether or not to enable a database locally in the cluster or not. In our deployment, we did not want that - we wanted to use the Cloud SQL database that we already created.</p> <p>The <code>postgresqlHostname:</code> setting will be the internal IP address of your PostgreSQL instance.</p> <p>Remember that we created a separate database for each platform we are running, so we changed the <code>database:</code> and <code>username:</code> to match what we created. Because we are also using a different user for the platform database, we needed to set both the <code>password:</code> (which is the password of the account in the <code>username:</code> setting) and the <code>postgresPassword:</code> (which is the password of the default <code>postgres</code> account) to the correct values. Mastodon uses each for different database tasks, so it needs both passwords in this configuration.</p> <p>When you get to the actual deployment and your pods spin up, you should notice a Job called <code>mastodon-db-migrate</code> spin up as well. This job is creating the correct database schema for your instance. Your other Mastodon pods may not be available until that job completes.</p> <p>Note</p> <p>You may find that the <code>mastodon-db-migrate</code> job doesn't run with <code>postgres.enabled</code> set to <code>false</code> (although our experience was based on incorrectly setting it to <code>true</code> in our initial deployment and desperately trying to switch to Cloud SQL after deploying Mastodon1 ).</p> <p>YMMV with a correctly configured fresh install, but if that happens to you, here is how we fixed it. The <code>mastodon-web</code> and <code>mastodon-sidekiq</code> pods will fail to start, but the <code>mastodon-streaming</code> pod will because, unlike the other two, it is not dependent on a database connection. The dirty little secret is that all three pods are running the same Mastodon image, so we can use the running <code>mastodon-streaming</code> pod to access a running Mastodon service and the <code>rails</code> environment we need.</p> <p>Connect to the running <code>mastodon-streaming</code> pod from your CLI machine by entering this:</p> <p><pre><code>~$ kubectl get pods -n mastodon\n</code></pre> <pre><code>NAME                                          READY   STATUS\nmastodon-streaming-bb578b4bc-gzfr2            1/1     Running\nmastodon-streaming-bb578b4bc-nszjf            1/1     Running\nmastodon-streaming-bb578b4bc-wsv2l            1/1     Running\n</code></pre> <pre><code>~$ kubectl exec -it mastodon-streaming-bb578b4bc-wsv2l -n mastodon /bin/sh\n</code></pre></p> <p>It doesn't matter which pod you connect to, if there is more than one, like in the example above. Once you are connected to a pod, run the following:</p> <pre><code>$ export OTP_SECRET=[redacted]\n$ export SECRET_KEY_BASE=[redacted]\n$ RAILS_ENV=production bundle exec rails db:migrate\n</code></pre> <p>You should see the <code>mastodon-db-migrate</code> job appear in your <code>Workloads</code> in the Cloud Console, and this will prepare your database. Once the job is finished, your <code>mastodon-web</code> and <code>mastodon-sidekiq</code> pods should restart successfully.</p>"},{"location":"building/mastodon/#helmrelease","title":"HelmRelease","text":"<p>Now that we have our instance-specific ConfigMap in place, we can create the HelmRelease that will pull the Helm chart from the repository we chose for it and apply our ConfigMap to it. Here is the HelmRelease file we use:</p> /mastodon/helmrelease-mastodon.yaml<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta1\nkind: HelmRelease\nmetadata:\nname: mastodon\nnamespace: mastodon\nspec:\nchart:\nspec:\nchart: ./\nsourceRef:\nkind: GitRepository\nname: mastodon\nnamespace: flux-system\ninterval: 15m\ntimeout: 5m\nreleaseName: mastodon\nvaluesFrom:\n- kind: ConfigMap\nname: mastodon-helm-chart-value-overrides\nvaluesKey: values.yaml </code></pre> <p>With all this in place, here is the sequence of events:</p> <ul> <li>The <code>/clusters/{my-cluster}/kustomizations/kustomization-mastodon.yaml</code> Kustomization tells Flux on that cluster to monitor the <code>./mastodon</code> path our repository (<code>name: flux-system</code>) for changes every <code>interval: 15m</code></li> <li>When a change is detected, it updates the <code>name: mastodon-helm-chart-value-overrides</code> ConfigMap from our repo on the cluster, and...</li> <li>pulls the <code>name: mastodon</code> HelmRelease, which points at the platform repository (<code>name: mastodon</code>) containing the Helm chart that builds the pods and services deploys the chart in our cluster, with their local <code>.env.production</code> files built from the updated ConfigMap.</li> <li>The <code>healthChecks</code> in the Kustomization are run to make sure everything deployed successfully.</li> </ul> <p>Note</p> <p>The platform repository is itself monitored for changes every <code>interval: 15m</code>, and changes in that will also trigger a Flux reconcilliation. If you want to avoid unexpected upgrades, you can specify a valid <code>image.tag</code> in your ConfigMap. This is particularly important now that v4.1 is imminent, and the published Helm Chart could change without warning.</p>"},{"location":"building/mastodon/#deploying-mastodon","title":"Deploying Mastodon","text":"<p>You can either wait for Flux to detect your changes, or you can speed up the process by running the following from your CLI machine:</p> <pre><code>flux reconcile source git flux-system\n</code></pre> <p>You can see the Mastodon pods by running the following from your CLI machine (or looking in your GKE console):</p> <p><pre><code>kubectl get pods -n mastodon\n</code></pre> <pre><code>mastodon-redis-master-0                       1/1     Running\nmastodon-redis-replicas-0                     1/1     Running\nmastodon-redis-replicas-1                     1/1     Running\nmastodon-redis-replicas-2                     1/1     Running\nmastodon-sidekiq-all-queues-7d8b8c596-456kx   1/1     Running\nmastodon-sidekiq-all-queues-7d8b8c596-97rfv   1/1     Running\nmastodon-sidekiq-all-queues-7d8b8c596-98d2x   1/1     Running\nmastodon-streaming-5bdc55888b-95fbn           1/1     Running\nmastodon-streaming-5bdc55888b-cwnsv           1/1     Running\nmastodon-streaming-5bdc55888b-czh28           1/1     Running\nmastodon-web-56cc95dd99-k7mfg                 1/1     Running\nmastodon-web-56cc95dd99-n524q                 1/1     Running\nmastodon-web-56cc95dd99-vmkxf                 1/1     Running\n</code></pre></p> <p>That's it! You're done deploying your Mastodon instance on GKE! Now, we need to make sure people can access it.</p>"},{"location":"building/mastodon/#ingress_1","title":"Ingress","text":"<p>You will remember that we did not enable the ingress that is included in the Mastodon Helm chart and instead opted to configure the GKE Ingress by hand.</p> <p>You can do this in the console by going to <code>Services &amp; Ingress</code> in the GKE menu in Google Cloud Console. You will need an <code>External HTTPS Ingress</code> with two ingress paths to make Mastodon work properly, especially with mobile applications:</p> <ul> <li>A path for the <code>mastodon-streaming</code> service, with the path set to <code>/api/v1/streaming</code></li> <li>A path for the <code>mastodon-web</code> service, with the path set to <code>/*</code></li> </ul> <p>As part of creating an HTTPS ingress, you will need a TLS certificate. We opted to use a Google-manged certificate. The domain for the certificate needs to be for the <code>web_domain</code> of the instance (or <code>local_domain</code>, if <code>web_domain</code> is not set).</p> <p>The <code>spec</code> for the resulting ingress should look like this:</p> <pre><code>spec:\nrules:\n- http:\npaths:\n- backend:\nservice:\nname: mastodon-web\nport:\nnumber: 3000\npath: /*\npathType: ImplementationSpecific\n- backend:\nservice:\nname: mastodon-streaming\nport:\nnumber: 4000\npath: /api/v1/streaming\npathType: ImplementationSpecific\n</code></pre> <p>The Ingress will be allocated an external IP address that you should add as an <code>A</code> record for the instance hostname in your DNS record set. Your TLS certificate will not validate until that DNS record propogates (usually within half an hour or so).</p> <p>Once it's all up and running, you should be able to connect to your instance from your web browser!</p>"},{"location":"building/mastodon/#load-balancer","title":"Load Balancer","text":"<p>For GOVSocial.org, we wanted our user accounts to have the scheme <code>{username}@govsocial.org</code>, rather than having the full domain of each platform in them, like <code>{username}@{platform}.govsocial.org</code>. This means that our <code>local_domain</code> in Mastodon is <code>govsocial.org</code>, while our <code>web_domain</code> is <code>mastodon.govsocial.org</code>.</p> <p>This poses challenges for federation, as any links to user profiles on other instances will intially connect to <code>govsocial.org</code> (the <code>local_domain</code>) instead of our <code>web_domain</code> and will need to be redirected. This redirection is handled by a <code>webfinger</code> service in Mastodon.</p> <p>The load balancer needs to redirect requests for <code>govsocial.org/.well-known/webfinger</code> (which is where other instances think it is based on our username scheme) to <code>mastodon.govsocial.org/.well-known/webfinger</code> (where the service is actually listening).</p> <p>To do this, we deployed an <code>HTTPS (classic) Load Balancer</code> in the <code>Network Services -&gt; Load Balancing</code> menu in our Google Cloud Console. The setup is very similar to the Ingress we set up earlier. In fact, you will see the load balancer created by the Ingress in the list when you go there.</p> <p>Warning</p> <p>Don't mess with it :-) If you're the sort of person who can't help pressing big red buttons that say \"DO NOT PRESS\", it's okay - anything you change will eventually be reverted by the GKE configuration, but your ingress might be broken until that happens.</p> <p>Create a new <code>HTTPS (classic) Load Balancer</code> (this one supports the hostname and path redirect features we need). Despite the higher cost, we selected the Premium network tier, as it allows for the addition of services like Cloud Armor and Cloud CDN if the platform needs it in the future.</p> <p>For the Frontend configuration, make sure to create a new fixed external IP address and add the corresponding <code>A</code> record for <code>local_domain</code> in your DNS record set. Because we want to use HTTPS, you will need to create a TLS certificate for your <code>local_domain</code>. The certificate won't validate until this DNS record propogates (usually with half an hour or so).</p> <p>For the Backend configuration, pick the default <code>kube-system-default-http-backend-80</code> service (there will be a bunch of letters/numbers before and after it.) This service doesn't have anything listening on it, but it will be used for the mandatory default rule in the rules configuration.</p> <p>In the <code>Host and path rules</code>, create a <code>Prefix redirect</code> for your <code>local_domain</code>, set the <code>Host redirect</code> to your <code>web_domain</code>, and the <code>Paths</code> to <code>/.well-known/webfinger</code>. Select <code>301 - Moved Permanently</code> for the response, and make sure that <code>HTTPS redirect</code> is enabled.</p> <p>Save your configuration, and wait for it to become available in the Cloud Console. Once it does, you should be able to connect to your Mastodon instance in your browser, and start operating it.</p> <p>Note</p> <p>One of the advantages of having this load balancer in conjunction with our domain scheme is that it means that we can use the default path (<code>/*</code>) of GOVSocial.org for documentation and non-instance specific content. We created a similar rule in our load balancer for <code>/*</code> that redirects to <code>docs.govsocial.org</code>, which is what you are reading now. There is a whole other write-up for how we created that!</p> <ol> <li> <p>Flux to the rescue again! This was one of the issues (the other was abandoning AutoPilot) that had us delete all the Mastodon workloads and start over. The postgreSQL configuration seems to be particularly \"sticky\" and, try as we might, we could not get the corrected configuration to take after the initial deployment.\u00a0\u21a9</p> </li> </ol>"},{"location":"building/postgresql/","title":"PostgreSQL","text":"<p>As with S3-compatible storage, most Fediverse platforms require a PostgreSQL database instance where much of the instance data is stored. Despite the importance of the performance of the database to that of the overall instance, you can save a lot of up-front cost by starting small and cloning to a larger machine as you grow.</p> <p>We use a Google CloudSQL PostgreSQL instance, selecting the smallest possible machine (<code>db-f1-micro</code>) and set the storage to <code>auto-grow</code>.</p> <p>You will need to ensure that you enable the Private IP interface for the instance so the GKE cluster can reach it. We did not enable the CloudSQL Auth Proxy - we may eventually do that at some point.</p> <p>We chose to create a separate database and user for each platform, rather than piling everything into the default <code>postgres</code> one.</p>"},{"location":"building/storage/","title":"S3 Storage","text":"<p>Most Fediverse platforms require some form of S3-compatible storage for images and other media files.</p> <p>We use Google Cloud Storage for this. Your bucket will need to be <code>public</code>, with <code>fine-grained</code> access control, and use the <code>standard</code> storage class.</p> <p>The service account principal doesn't need any roles - you'll use a HMAC key for access.</p> <p>Warning</p> <p>Store this key safely - you will only be shown it once at creation time.</p>"},{"location":"operating/","title":"Operating GOVSocial","text":"<p>Building our Fediverse instance was one thing, but we also learned a lot about operating them as well, especially in the \"safe for work\" mode and the culture of openness and transparency that are fundamental values of the public service. We are public servants ourselves, and embrace the principle of \"Default to Open\" from the US Digital Service Playbook.</p> <p>This created three clear goals for the GOVSocial project:</p> <ol> <li>To provide a comprehensive set of documentation detailing exactly how the platforms were built, so individual and agency users alike could see behind our technological, operational, and financial curtains.</li> <li>To provide assurance that account registration on our instances is, to the best of our ability and within the constraints of the deployed technology, accurately tied to transparent identification and validation of public service users and agencies.</li> <li>To provide, to the best of our ability and within the constraints of the deployed technology, a comprehensive and transparent user and server moderation policy, to shield our users from association with federated and local content that could harm the reputation of themselves or their agencies.</li> </ol> <p>Each of these goals presented some technical and operation challenges. In this section, we have documented the approaches and techniques we adopted to meet them.</p>"},{"location":"operating/documentation/","title":"GOVSocial Documentation","text":"<p>One of our goals from the outset of the project was to contribute to the Fediverse community by providing comprehensive documentation of our implementation and operation journey for others who wanted to deploy on GKE in a similar way that we did. As part of our initial research, we found few examples of instances that had been built entirely on Google Kubernetes Engine (GKE) and other Google Cloud Platform services.</p> <p>We had a couple of requirements for our document site:</p> <ul> <li>It needed to use MarkDown. We wanted to use an open standard, and generate our site from easily portable text documents, in case we wanted to change our hosting platform.</li> <li>It needed to be a static site. We didn't want a CMS with proprietary databases or plugins.</li> <li>It needed to be independently hosted, but not self-hosted. We wanted the freedom of hosting the site outside of an proprietary online CMS ecosystem, without the headaches of implementing, securing, and managing our own HTTP server.</li> </ul>"},{"location":"operating/documentation/#mkdocs","title":"MkDocs","text":"<p>After some research, we settled on MkDocs-Material as our static content generator. The Funky Penguin's Geek Cookbook uses it, and it suits the creation and maintenance of technical documentation very well. It is powerful, but also simple to get started with1.</p> <p>We primarily use Chromebooks as our workstations, which presents a challenge when it comes to installing IDE tools locally. Enter GitHub Codespaces2. Every personal and organizational repository on GitHub comes with a basic amount (60 hours and 15GB per month) of Codespaces, allowing any device with Internet access to run an IDE of choice.</p> <p>Even better, you can create your Codespace from an existing GitHub repo, which is exactly what we did.</p>"},{"location":"operating/documentation/#github-repository","title":"GitHub Repository","text":"<p>We forked the MkDocs-Material into our own repo, named so that we could use GitHub Pages to host it.</p> <p>From our repo, we created our Codespace, following these instructions from MkDocs's own excellent documentation. Notice that we forked the repo rather than cloning it - we wanted to be able to sync with the original repo from time to time to keep things up to date3.</p> <p>Your GitHub Codespace will prompt you for some Python extensions, which you should install, but apart from that, getting it up and running is very straightforward. We have left it as an exercise for you, dear reader, to learn MarkDown and MkDocs-Material from their own excellent documentation sites.</p> <p>You can preview your site as you create it by typing <code>mkdocs serve</code> in your Codespace terminal, and clicking on <code>Open in Browser</code> in the dialog that appears.</p>"},{"location":"operating/documentation/#github-pages","title":"GitHub Pages","text":"<p>As mentioned above, we decided to go all in with GitHub for the site and use GitHub Pages to host it. To create a GitHub pages site, all you need to do is name your repo using the <code>[user|organization].github.io</code> naming convention. Your repo can either be public or private, depending on your preference.</p> <p>Integrating with MkDocs website deployment is a snap. Commit your local changes to your repo, and then run <code>mkdocs gh-deploy --force</code> from your Codespace. This will force push everything in the <code>docs</code> (the MkDocs default) directory into a <code>gh-pages</code> branch in your repo.</p> <p>Configure your GitHub Pages (in Settings) to publish from the <code>gh-pages</code> branch of your repo, and you should see your content published at <code>https://[user|organization].github.io</code> within a minute or so.</p> <p>Note</p> <p>The MkDocs documentation suggests examples of ways of automating the deployment of your site each time you commit changes to your repo. We opted to deploy manually, as we wanted to publish to the public site when we wanted to, and not every time someone committed a change. YMMV.</p>"},{"location":"operating/documentation/#custom-domain","title":"Custom Domain","text":"<p>You will remember that we created a load balancer to direct all paths other than the one needed for WebFinger to <code>docs.govsocial.org</code>. Using this for our GitHub Pages site was striaghtforward as well:</p> <ul> <li>Create a <code>CNAME</code> entry in your Cloud DNS record set that redirects your custom documentation hostname (<code>docs.govsocial.org</code>, in our case) to your GitHub Pages hostname (<code>[user|organization].github.io</code>).</li> <li>Next, and this is an important if minor step that we initially missed, create a file called <code>CNAME</code> in at the top of your <code>docs</code> directory structure. This file should contain your custom domain in it, and nothing else, like this: CNAME<pre><code>docs.govsocial.org\n</code></pre> When you complete the next step, GitHub automatically creates this file in the <code>gh-pages</code> branch of your repo, and it will get clobbered by a deployment if you haven't added it to your local copy.</li> <li>Finally, go to the Settings for your GitHub pages, and enter your custom domain in the appropriate place.</li> </ul> <p>Once this all syncs up, and if you have been following all the steps we took, your naked domain should redirect to your custom docs domain, and that in turn will redirect to your GitHub pages site!</p> <ol> <li> <p>Full disclosure: we unabashedly copied a lot of what the Cookbook site uses for their site - imitation is the sincerest form of flattery :-).\u00a0\u21a9</p> </li> <li> <p>Hat-tip to Elan@PublicSquare.global for suggesting this.\u00a0\u21a9</p> </li> <li> <p>It turns out that the MkDocs-Material repo is very active, and we quickly ended up in a situation where we need to merge changes rather than automatically syncing them. This is on our TO-DO list.\u00a0\u21a9</p> </li> </ol>"},{"location":"operating/mastodon/","title":"Mastodon Operations","text":"<p>Now that you have your Mastodon instance built, you're going to want to start using it!</p>"},{"location":"operating/mastodon/#administrator-access","title":"Administrator Access","text":"<p>If you recall, the admin user that we specified in our Mastodon configuration didn't get created when we deployed our instance. Here's how we worked around that:</p> <ul> <li>Register the account you want to act as the owner of the instance, as a regular user, including confirming your email address.</li> <li>Connect to a running <code>mastodon-web</code> pod from your CLI machine by entering this: <pre><code>~$ kubectl get pods -n mastodon\n</code></pre> <pre><code>NAME                                          READY   STATUS\nmastodon-media-remove-27895680-w6f95          0/1     Completed\nmastodon-media-remove-27905760-2jnnd          0/1     Completed\nmastodon-media-remove-27915840-mprc7          0/1     Completed\nmastodon-redis-master-0                       1/1     Running\nmastodon-redis-replicas-0                     1/1     Running\nmastodon-redis-replicas-1                     1/1     Running\nmastodon-redis-replicas-2                     1/1     Running\nmastodon-sidekiq-all-queues-968db8b7c-77wjg   1/1     Running\nmastodon-sidekiq-all-queues-968db8b7c-hh5kz   1/1     Running\nmastodon-sidekiq-all-queues-968db8b7c-l7bjc   1/1     Running\nmastodon-streaming-bb578b4bc-6pv56            1/1     Running\nmastodon-streaming-bb578b4bc-7lc68            1/1     Running\nmastodon-streaming-bb578b4bc-swjdh            1/1     Running\nmastodon-web-6b7fb7f8d4-2d7gz                 1/1     Running\nmastodon-web-6b7fb7f8d4-ds5vm                 1/1     Running\nmastodon-web-6b7fb7f8d4-dvr4j                 1/1     Running\n</code></pre> <pre><code>~$ kubectl exec -it mastodon-web-6b7fb7f8d4-dvr4j -n mastodon /bin/sh\n</code></pre></li> <li>It doesn't matter which <code>mastodon-web</code> pod you connect to, if there is more than one, like in the example above. Once you are connected to a pod, run the following: <pre><code>$ RAILS_ENV=production bin/tootctl accounts modify {username} --role Owner\n</code></pre></li> </ul> <p>This will give your account owner and admin access to your Mastodon instance in the browser. The <code>Trends</code>, <code>Moderation</code>, <code>Administration</code>, <code>SideKiq</code>, and <code>PGHero</code> menu items are added to the menu in your regular user settings. Get used to keeping an eye on these regularly! A couple of general administrative tips:</p> <ul> <li>Be sure to walk through the <code>Server Settings</code> and set up, at a minumum, your <code>About</code> message and your <code>Server Rules</code>. Ours are here.</li> <li>It is strongly advised that you add a trusted friend as another instance administrator. Until you do, you are the only person with that access. You will also use the <code>Roles</code> menu to manage your growing team of administrators and moderators as your instance grows.</li> <li>Keep an eye on your Sidekiq queues, especially anything in <code>Retries</code>. There will usually be some sort of error message for those that will help you troubleshoot the issue, particularly if your SMTP configuration isn't working.</li> <li>The first time you look at <code>PGHero</code>, it will tell you that the <code>pg-stats</code> extension needs to be enabled. Do this. Google Cloud SQL installs this extension by default, and if you followed the steps in our Postgres implementation, you should be able to install it right from the browser. This will alert you to any slow-running queries in your database.</li> </ul>"},{"location":"operating/mastodon/#backups","title":"Backups","text":"<p>Upholding the Mastodon Server Covenant means ensuring that our instance availability and configuration and our users' data must protected from loss. This means ensuring regular and frequent backups of:</p> <ul> <li>Our PostgreSQL database. We perform automated nightly full database backups using Cloud SQL backups, with a 7-day retention. We have also enabled point-in-time recovery with 7-day log retention.</li> <li>Our Cloud Storage buckets. Google Cloud Storage is geo-redundant, meaning that data is redundant across at least two zones within at least one geographic place as soon as our users upload it. Our Cloud Storage is used solely for user-uploaded media, such as avatars, banners, and media attached to posts. We believe that geo-redundancy is sufficient to protect this data from loss.</li> <li>GKE Clusters and persistent volume (PV) claims. We perform automated nightly cluster backups of our instance namespaces using Backup for GKE, with a 7-day retention period.</li> </ul> <p>Warning</p> <p>Backup for GKE restores all the pods in the the scope of the backup plan, regardless of status. If, like we did, you created a lot of terminated and failed pods during your initial deployment, you will want to clear them out to minimize your backup costs. You can get a list of any such pods by running this in from your CLI machine:</p> <pre><code>~$ kubectl get pods --field-selector status.phase=Failed -A\n</code></pre> <p>You can delete the failed pods by running this from your CLI machine: <pre><code>~$ kubectl delete pods --field-selector status.phase=Failed -A\n</code></pre></p>"},{"location":"operating/mastodon/#account-registration-approval","title":"Account Registration Approval","text":"<p>If you leave account registration open on your Mastodon instance this section won't apply, but we wanted to limit accounts on our platform instances to identified and verified public service users and agencies. In order to do this, we require that users sign up with their organizational email address, which we verify against information on the organization's website, among other sources.</p> <p>We wanted to automate this process as much as we could, partly to make the registration process as seamless as possible for our intended user community, and partly to reduce the administative burden of having to approve every single user.</p> <p>Many national governments own and administer a top-level or secondary-level DNS domain, use of which already requires that the organization to which a subdomain is being issued meets our criteria for a public service body. These include1:</p> <ul> <li>.gov</li> <li>.gov.au</li> <li>.gc.ca</li> <li>.gov.uk</li> </ul> <p>There are also free personal email domains that we are reasonably certain are never used by public service entities. These include2:</p> <ul> <li>gmail.com</li> <li>yahoo.com</li> <li>aol.com</li> </ul> <p>We automated the handling of both of these lists with a Tines workflow that uses the Mastodon API to interact with our instance.</p>"},{"location":"operating/mastodon/#tines-automation","title":"Tines Automation","text":"<p>In order to automate the processing of registration requests, you will need:</p> <ul> <li>A free Tines account.</li> <li>An OAuth token for an account in your Mastodon instance with the <code>admin:read:accounts</code> and <code>admin:write:accounts</code> scopes.</li> </ul>"},{"location":"operating/mastodon/#mastodon-oauth-token","title":"Mastodon OAuth Token","text":"<p>To generate the OAuth token for your Mastodon account, go to the <code>Development</code> item in your Mastodon account settings. Click <code>New Application</code>, and enter a suitable account name (eg. <code>Tines</code>). When you signed up for Tines and created your tenant, it was given a URL - enter that in the <code>Application website</code> field. Leave the <code>Redirect URI</code> field at its default setting for now.</p> <p>Scroll down the list of <code>Scopes</code> until you see <code>admin:read:accounts</code> and <code>admin:write:accounts</code> and check those. You can uncheck everything else in the <code>Scopes</code> area.</p> <p>When you're done, click <code>Submit</code>. Edit your newly created application by clicking on it. This will generate and display the OAuth token that you need at the top of the page. The <code>Client key</code> and <code>Client secret</code> are the values you will need for creating the Tines credential. You don't have to write these down - you'll be able to access them from this menu whenever you need them. Leave this screen open, and switch to your Tines tenant in a different browser tab.</p>"},{"location":"operating/mastodon/#tines-credential","title":"Tines Credential","text":"<p>In order for your Tines workflow to authenticate to your Mastodon instance, it will need a credential. You can create one in your Tines team (everyone gets a team in Tines, even if they are an individual user) by expanding your team name and clicking on <code>Credentials</code> and then <code>New Credential</code> in the screen that appears. Pick <code>OAuth 2.0</code> from the dropdown menu.</p> <p>Give your credential a suitable name, and then get ready to switch back and forth between this screen and your Mastodon <code>Development</code> screen as you set this up.</p> <p>The Tines screen will give you a <code>Callback URL</code>. Copy this and paste it into the <code>Redirect URI</code> field in your Mastodon screen. While you are in the Mastodon screen, copy and paste both the <code>Client key</code> and <code>Client secret</code> from there into the <code>Client ID</code> and <code>Client secret</code> fields respectively in your Tines screen. When you're done, click <code>Save</code> in the Mastodon screen, but leave that tab open.</p> <p>Switch back to Tines, and enter the scopes into the <code>Scopes</code> field, exactly like this:</p> <pre><code>admin:read:accounts admin:write:accounts\n</code></pre> <p>Set the remaining values in the Tines screen as follows:</p> <ul> <li>Set <code>OAuth Provider</code> to <code>Manual</code></li> <li>Set <code>Grant type</code> to <code>Authorization code</code></li> <li>Set <code>OAuth authorization request URL</code> to <code>https://{instance-web_domain-value}/oauth/authorize</code></li> <li>Set <code>PKCE challenge method</code> to <code>SHA-256</code></li> <li>Set <code>OAuth token URL</code> to <code>https://{instance-web_domain-value}/oauth/token</code></li> <li>Set <code>Domains</code> to <code>{instance-web_domain-value}</code></li> <li>Make sure that <code>All teams &amp; drafts</code> is checked</li> </ul> <p>When you click <code>Save</code>, Tines will generate its OAuth token and, in your Mastodon screen, you should see a request for access which you can accept. Once that is complete, your OAuth token is ready for use in your Tines workflow.</p>"},{"location":"operating/mastodon/#tines-workflow","title":"Tines Workflow","text":"<p>Workflows in Tines are called \"Stories\", so click on the <code>Stories</code> item in the <code>Your drafts</code> part of your Tines menu. To make it easier for you to set everything up, we have exported our Tines workflow as a JSON file, which you can save locally and import using the <code>Import</code> button in this screen3.</p> <p>Note</p> <p>You should replace <code>{my-mastodon-web_domain}</code>, <code>{my-credential}</code>, and <code>{my-email_address}</code> with your own values directly in the file before importing it. Remember to get rid of the braces <code>{}</code> as well!</p> <p>When you have saved and tested your draft Story, you can publish it in Tines, and you should be all set!</p> Tines Workflow <pre><code>{\n\"schema_version\": 6,\n\"standard_lib_version\": 13,\n\"action_runtime_version\": 1,\n\"name\": \"Auto-approve Mastodon accounts\",\n\"description\": \"Auto-approves pending Mastodon accounts with email addresses in known domains\",\n\"guid\": \"b9700ef606a5f4badfed04ea42155b12\",\n\"slug\": \"auto_approve_mastodon_accounts\",\n\"exported_at\": \"2023-02-02T21:10:22Z\",\n\"agents\": [\n{\n\"type\": \"Agents::HTTPRequestAgent\",\n\"name\": \"Get Pending User Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"db1336c590099940d47bb2389b634d2b\",\n\"options\": {\n\"url\": \"https://{my-mastodon-web_domain}/api/v2/admin/accounts\",\n\"content_type\": \"application_json\",\n\"method\": \"get\",\n\"payload\": {\n\"origin\": \"local\",\n\"status\": \"pending\",\n\"limit\": 200\n},\n\"headers\": {\n\"Authorization\": \"Bearer &lt;&lt;CREDENTIAL.{my-credential}&gt;&gt;\"\n}\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": [\n{\n\"cron\": \"*/5 * * * *\",\n\"timezone\": \"America/Detroit\"\n}\n]\n},\n{\n\"type\": \"Agents::EventTransformationAgent\",\n\"name\": \"Extract Pre-Approved Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"b2f9f20787f3decfe59cc4f5edb041d9\",\n\"options\": {\n\"mode\": \"message_only\",\n\"loop\": false,\n\"payload\": {\n\"pre-approved_accounts\": \"=FILTER(get_pending_user_accounts.body, LAMBDA(element, MATCH(element.email,\\\"(.*\\\\.gov|.*\\\\.gov\\\\.au|.*\\\\.gov\\\\.uk|.*\\\\.gc\\\\.ca)\\\")))\"\n}\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": null\n},\n{\n\"type\": \"Agents::EventTransformationAgent\",\n\"name\": \"Explode Pre-Approved Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"4368d4a7e09e8b0ba1278435502d6c58\",\n\"options\": {\n\"mode\": \"explode\",\n\"path\": \"=extract_pre_approved_accounts[\\\"pre-approved_accounts\\\"]\",\n\"to\": \"individual_item\"\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": null\n},\n{\n\"type\": \"Agents::HTTPRequestAgent\",\n\"name\": \"Approve Pre-Approved Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"9e5ad0f2d078d50408a3a91e643c8f40\",\n\"options\": {\n\"url\": \"https://{my-mastodon-web_domain}/api/v1/admin/accounts/&lt;&lt;explode_pre_approved_accounts.individual_item.id&gt;&gt;/approve\",\n\"content_type\": \"application_json\",\n\"method\": \"post\",\n\"headers\": {\n\"Authorization\": \"Bearer &lt;&lt;CREDENTIAL.{my-credential}&gt;&gt;\"\n}\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": null\n},\n{\n\"type\": \"Agents::EventTransformationAgent\",\n\"name\": \"Extract Pre-Rejected Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"fcfe50b2803bc32077c04dce841a33f2\",\n\"options\": {\n\"mode\": \"message_only\",\n\"loop\": false,\n\"payload\": {\n\"pre-rejected_accounts\": \"=FILTER(get_pending_user_accounts.body, LAMBDA(element, MATCH(element.email,\\\"(.*\\\\gmail\\\\.com|.*\\\\aol\\\\.com|.*\\\\yahoo\\\\.com)\\\")))\"\n}\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": null\n},\n{\n\"type\": \"Agents::EventTransformationAgent\",\n\"name\": \"Explode Pre-Rejected Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"5c672894c3e6af04e43a7e5b69de1bd9\",\n\"options\": {\n\"mode\": \"explode\",\n\"path\": \"=extract_pre_rejected_accounts[\\\"pre-rejected_accounts\\\"]\",\n\"to\": \"individual_item\"\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": null\n},\n{\n\"type\": \"Agents::HTTPRequestAgent\",\n\"name\": \"Reject Pre-Rejected Accounts\",\n\"disabled\": false,\n\"description\": \"\",\n\"guid\": \"c1958d276321136a1a86833ec33a87ea\",\n\"options\": {\n\"url\": \"https://{my-mastodon-web_domain}/api/v1/admin/accounts/&lt;&lt;explode_pre_rejected_accounts.individual_item.id&gt;&gt;/reject\",\n\"content_type\": \"application_json\",\n\"method\": \"post\",\n\"headers\": {\n\"Authorization\": \"Bearer &lt;&lt;CREDENTIAL.{my-credential}&gt;&gt;\"\n}\n},\n\"reporting\": {\n\"time_saved_value\": 0,\n\"time_saved_unit\": \"minutes\"\n},\n\"monitoring\": {\n\"monitor_all_events\": false,\n\"monitor_failures\": false,\n\"monitor_no_events_emitted\": null\n},\n\"width\": null,\n\"schedule\": null\n}\n],\n\"diagram_notes\": [],\n\"links\": [\n{\n\"source\": 0,\n\"receiver\": 1\n},\n{\n\"source\": 0,\n\"receiver\": 4\n},\n{\n\"source\": 1,\n\"receiver\": 2\n},\n{\n\"source\": 2,\n\"receiver\": 3\n},\n{\n\"source\": 4,\n\"receiver\": 5\n},\n{\n\"source\": 5,\n\"receiver\": 6\n}\n],\n\"diagram_layout\": \"{\\\"db1336c590099940d47bb2389b634d2b\\\":[495,45],\\\"b2f9f20787f3decfe59cc4f5edb041d9\\\":[330,135],\\\"4368d4a7e09e8b0ba1278435502d6c58\\\":[330,225],\\\"9e5ad0f2d078d50408a3a91e643c8f40\\\":[330,330],\\\"fcfe50b2803bc32077c04dce841a33f2\\\":[585,135],\\\"5c672894c3e6af04e43a7e5b69de1bd9\\\":[585,225],\\\"c1958d276321136a1a86833ec33a87ea\\\":[585,330]}\",\n\"send_to_story_enabled\": false,\n\"entry_agent_guid\": null,\n\"exit_agent_guids\": [],\n\"exit_agent_guid\": null,\n\"keep_events_for\": 604800,\n\"reporting_status\": true,\n\"send_to_story_access\": null,\n\"story_library_metadata\": {},\n\"recipients\": [\n\"{my-email_address}\"\n],\n\"story_level_monitoring_enabled\": false,\n\"monitor_failures\": false,\n\"send_to_stories\": [],\n\"form\": null,\n\"forms\": []\n}\n</code></pre>"},{"location":"operating/mastodon/#server-moderation","title":"Server Moderation","text":"<p>As part of creating an instance safe for public service users and agencies using their public personas, it is important that we can maintain a reliable server moderation policy, limiting federation with instances that are conflict with our server rules.</p> <p>Also, as a small team, we wanted to automate the maintainance of our base server moderation list from these sources, to allow us to dedicate our resources to the moderation of our specific instance. Enter the FediBlockHole project4.</p>"},{"location":"operating/mastodon/#fediblockhole","title":"FediBlockHole","text":"<p>We have documented how we containerized and deployed FediBlockHole in our cluster here. Once that was done, we needed to decide which lists to pull from.</p>"},{"location":"operating/mastodon/#selecting-block-lists","title":"Selecting Block Lists","text":"<p>Server moderation in the Fediverse is a little like spam filtering for email servers. It is generally up to each individual mailop to decide and implement their own spam filtering policy, with widely varying results. Over time, trusted lists of \"known bad\" servers (such as these) have evolved to provide a more consistent and centralized way of filtering traffic from poorly-behaved MTAs.</p> <p>The RapidBlock Project is an equivalent of this for Mastodon instances, and was a clear choice for us. You can see where we pull this list in our FediBlockHole configuration file here:</p> <pre><code>blocklist_url_sources = [\n# { url = 'file:///path/to/fediblockhole/samples/demo-blocklist-01.csv', format = 'csv' },\n{ url = 'https://rapidblock.org/blocklist.json', format = 'rapidblock.json' },\n]\n</code></pre> <p>The <code>rapidblock.json</code> format is specific to RapidBlock (it is designed for use by Fediverse platforms as a whole, and not just Mastodon), and looks like this:</p> <pre><code>{\n\"@spec\": \"https://rapidblock.org/spec/v1/\",\n\"publishedAt\": \"2022-12-29T18:40:02.065805293Z\",\n\"blocks\": {\n\"101010.pl\": {\n\"isBlocked\": true,\n\"reason\": \"cryptomining javascript, white supremacy\",\n\"tags\": [\n\"antisemitism\",\n\"malware\",\n\"racism\"\n],\n\"dateRequested\": \"2022-11-16T00:00:00Z\",\n\"dateDecided\": \"2022-11-16T00:00:00Z\"\n...\n</code></pre> <p>Note</p> <p>Mastodon server moderation has severity levels that are explained here, but you will notice from the sample RapidBlock file that no severity level is specified. This means that FediBlockHole will use the <code>max_severity</code> configuration setting, which is <code>suspend</code> by default and, if users on your instance are following accounts from the moderated domain, the <code>max_followed_severity</code> setting which defaults to <code>silence</code>.</p> <p>We also came across a set of consolidated blocklists created (using FediBlockHole) from the server block lists from various \"tiers\" of Mastodon instances. The Oliphant Social Blocklist provides a number of different lists using different tiers and merge plans for each.</p> <p>And here is where the fun started. Being new, and sensitive to our goal of creating an instance safe for public service users and agencies using their public personas, we initially selected the Unified Max Block List. This is all blocks from all tiers, using a <code>max</code> merge plan.</p> <p>This resulted in a huge blocklist, including the instance (mstdn.ca) where at least one of our team has their personal account! As noted here, FediBlockHole has no mechanism for removing blocks that are no longer in the subscribed lists, so we had to figure out what to do next.</p> <p>Luckily, there is a collection of Python-based utilities, from which we got a script that, once we added a timeout to avoid tripping the rate-limit on our API, did the trick.</p> <p>We now use the much more limited but still effective Unified Min Blocklist, which combines block lists from tiers 0 through 2 servers with a <code>min</code> merge plan. You can see where we pull this list in our FediBlockHole configuration file here:</p> <pre><code>blocklist_url_sources = [\n# { url = 'file:///path/to/fediblockhole/samples/demo-blocklist-01.csv', format = 'csv' },\n{ url = 'https://codeberg.org/oliphant/blocklists/raw/branch/main/blocklists/_unified_min_blocklist.csv', format = 'csv' },\n]\n</code></pre> <p>Note</p> <p>This list does NOT include the RapidBlock list, so we combine it with the above list, using a <code>max</code> merge plan</p>"},{"location":"operating/mastodon/#selecting-a-merge-plan","title":"Selecting a Merge Plan","text":"<p>One of the great features of FediBlockHole is the ability to specify a <code>mergeplan</code> setting of <code>min</code> or <code>max</code> when combining lists from multiple sources. The default is <code>max</code>.</p> <p>The difference between the two is how FediBlockHole resolves the conflict when the same block is encountered from different sources with different severities. A <code>max</code> merge plan selects the highest of the severity levels from all sources of the same block, while a <code>min</code> merge plan selects the lowest.</p> <p>We have opted to implement a <code>max</code> merge plan for our lists.</p> <ol> <li> <p>We are keen to add more - please let us know if there are any that should be added. Our worldview is regrettably US-centric, and there are likely many others that we have missed.\u00a0\u21a9</p> </li> <li> <p>This list is subject to change without notice, but we will keep our documentation of it, both here and on our instances, up to date.\u00a0\u21a9</p> </li> <li> <p>We haven't tried this ourselves so, if you do, let us know how it went!\u00a0\u21a9</p> </li> <li> <p>Hat tip to oliphant@oliphant.social for pointing us at this!\u00a0\u21a9</p> </li> </ol>"}]}